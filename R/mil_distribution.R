##' Function to construct mild object
##'
##' Function to construct mild object. `x` should be a list with components `model` (an smm object), `total_step` (a integer), `representative_inst` (a matrix of two columns), `traindata` (a data.frame with the first column being 'instance_name')
##' @param x A list
##' @return A mild object.
##' @examples
##' new_mild(list())
##' @export
##' @author Yifei Liu
new_mild <- function(x = list()) {
    stopifnot(is.list(x))
    structure(x, class = "mild")
}

##' Function to perform the SMM iteration using full Gram matrix.
##'
##' Internal function to perform SMM iteration using full Gram matrix.
##' @param kernel_full The full Gram matrix, should be of length n_inst * n_inst.
##' @param data_info the instance level data information which is a data.frame with 3 columns, 'bag_label', 'bag_name' and 'instance_name'
##' @param max.step maximum iteration steps
##' @param cost the cost used in SMM
##' @param weights Weights of each class
##' @param sigma the rbf kernel parameter
##' @param yy the response at instance level.
##' @param useful_inst_idx a vector specifying which indices are of use.
##' @return A list with several entries.
##'
##' @author Yifei Liu
kernel_mil <- function(kernel_full, data_info, max.step, cost, weights,
    sigma, yy, useful_inst_idx) {

    ## data_info is at instance_level
    bag_name <- data_info$bag_name
    bag_label <- data_info$bag_label
    positive_bag_name <- unique(bag_name[bag_label == 1])
    unique_bag_name <- unique(bag_name)
    n_bag <- length(unique_bag_name)  ## total number of bags

    len_y <- length(yy)

    selection <- rep(0, length(positive_bag_name))  ## this records which instance is selected in which bag
    past_selection <- matrix(NA, length(positive_bag_name), max.step)  ## this is the history of past selection.
    past_selection[, 1] <- selection

    yy_inst <- yy[useful_inst_idx]
    step <- 1
    while (step < max.step) {
        svm_model <- SMM(df = NULL, kernel_mild = kernel_full[useful_inst_idx,
            useful_inst_idx], cost = cost, class.weights = weights, sigma = sigma,
            y = yy_inst)
        ## need to get the predicted score for each instance of the original
        ## data. This should be carefully handled

        pred_all_score <- predict(object = svm_model, kernel_mild = kernel_full[useful_inst_idx, ])  ## uses lazy evaluation scheme in R

        ## update sample
        last_inst_idx <- 0
        pos_idx <- 1
        useful_inst_idx <- NULL  ## the same as the previous useful_inst_idx

        for (i in 1:n_bag) {
            data_i <- data_info[bag_name == unique_bag_name[i], ]
            n_inst_i <- nrow(data_i)  ## total number of instances

            if (data_i$bag_label[1] == 0) {
                ## negative bag
                useful_inst_idx <- c(useful_inst_idx, (last_inst_idx +
                  1):(last_inst_idx + n_inst_i))
            } else {
                id_max <- which.max(pred_all_score[(last_inst_idx + 1):(last_inst_idx +
                  n_inst_i)])
                selection[pos_idx] <- id_max
                useful_inst_idx <- c(useful_inst_idx, last_inst_idx +
                  id_max)
                pos_idx <- pos_idx + 1
            }
            last_inst_idx <- last_inst_idx + n_inst_i
        }

        ## if the selection is not changed, break.
        difference = sum(past_selection[, step] != selection)
        repeat_selection <- 0
        if (difference == 0)
            break

        ## if the current selection is the same as a previous one, break.
        repeat_selection <- 0
        for (i in 1:step) {
            if (all(selection == past_selection[, i])) {
                repeat_selection <- 1
                break
            }
        }
        if (repeat_selection == 1)
            break

        step <- step + 1
        past_selection[, step] <- selection
    }
    return(list(svm_model = svm_model, useful_inst_idx = useful_inst_idx,
        step = step, selection = selection, difference = difference,
        repeat_selection = repeat_selection, n_bag = n_bag))
}

##' Function to implement the iterative multiple instance learning with distributional data algorithm.
##'
##' Workhorse for the package. This implements the algorithm that iteratively find the representative positive instances and use smm to get the model.
##' @param data A MilData object, potentially generated by MilData().
##' @param cost The cost for smm.
##' @param max.step The total number of iterations.
##' @param sigma The parameter for the rbf kernel.
##' @return A mild object which contains the results.
##' @examples
##' MilData1 <- GenerateMilData(positive_dist = 'mvt',
##'                             negative_dist = 'mvnormal',
##'                             remainder_dist = 'mvnormal',
##'                             nbag = 10,
##'                             positive_degree = 3
##'                            )
##' foo <- mil_distribution(data = MilData1, cost = 1) ## uses about 10 seconds.
##'
##' @author Yifei Liu
mil_distribution_original_no_pre_kernel <- function(data, cost, max.step = 500,
    sigma = 0.05) {
    ## data should be of a MilData object.  bag_label should be one of '0'
    ## and '1', where '0' is negative bags and '1' is positive bags

    ## divide the bags to positive bags and negative bags

    bag_name <- data$bag_name
    bag_label <- data$bag_label
    instance_name <- unique(data$instance_name)
    if (length(unique(bag_label)) == 1)
        stop("Only one class label, cannot perform classification!")

    positive_bag_name <- unique(bag_name[bag_label == 1])
    negative_bag_name <- unique(bag_name[bag_label == 0])
    unique_bag_name <- unique(bag_name)
    n_bag <- length(unique_bag_name)  ## total number of bags

    ## Calculate the full kernel matrix, kernel_full is a
    ## length(instance_name) by length(instance_name) matrix.
    kernel_full <- kme(df = data, sigma = sigma)

    ## initialize the feature
    useful_inst_idx <- NULL  ## records which instances are used in the iterations, length not known in advance. This is at instance level, hence useful_inst_idx can never be larger than length(instance_name).
    yy <- NULL  ## Label at instance level. Always has the same length as useful_inst_idx.
    last_inst_idx <- 0  ## at the beginning of each loop, idx is at the end of the last instance in the previous bag.

    for (i in 1:n_bag) {
        ## loop over all unique bags.
        data_i <- data[bag_name == unique_bag_name[i], ]  ## find the i-th bag
        n_inst_i <- length(unique(data_i$instance_name))  ## total number of instances in i-th bag
        if (data_i$bag_label[1] == 0) {
            # if all instances in this bag has a negative label, then this is a
            # negative bag


            useful_inst_idx <- c(useful_inst_idx, (last_inst_idx + 1):(last_inst_idx +
                n_inst_i))  ## add instances in this bag to instance index
            yy <- c(yy, rep(0, n_inst_i))  ## all instances in this bag have negative label
            last_inst_idx <- last_inst_idx + n_inst_i  ## move last_inst_idx to the end of this bag.

        } else if (data_i$bag_label[1] == 1) {
            ## if the bag is a positive bag
            useful_inst_idx <- c(useful_inst_idx, last_inst_idx + 1)  ## initialize using the first instance in the positive bag.
            yy <- c(yy, rep(1, n_inst_i))
            last_inst_idx <- last_inst_idx + length(unique(data_i$instance_name))
        } else {
            stop(paste("Bag labeling is inconsitent for ", i, "-th bag, ",
                unique_bag_name[i]))
        }
    }
    yy <- factor(yy, levels = c(0, 1), labels = c("0", "1"))  ## change yy to a factor.
    num_neg_inst <- length(useful_inst_idx) - length(positive_bag_name)  ## calculate the number of negative instances.
    weights <- c(length(positive_bag_name)/num_neg_inst, 1)  ## this is less affected by the total number of bags.
    names(weights) <- c("0", "1")

    ## iterate between updating the model and selecting the most positive
    ## bag from an instance.
    selection <- rep(0, length(positive_bag_name))  ## this records which instance is selected in which bag
    past_selection <- matrix(NA, length(positive_bag_name), max.step)  ## this is the history of past selection.
    past_selection[, 1] <- selection
    ## step <- 1

    data_info <- unique(data[, c("bag_label", "bag_name", "instance_name")])
    temp_res <- kernel_mil(kernel_full, data_info, max.step, cost, weights,
        sigma, yy, useful_inst_idx)

    sample_df <- data[data$instance_name %in% instance_name[temp_res$useful_inst_idx],
        -c(1, 2)]
    res <- new_mild(list(model = temp_res$svm_model, total_step = temp_res$step,
        representative_inst = cbind(positive_bag_name, temp_res$selection),
        traindata = sample_df))
    return(res)
}

#' Initialize Instance Selection
#'
#' Use bag_label and instance_name information to initialize the selected
#' instances. When bag label is 0, select all instances.  When bag label is
#' 1, select the first instance in each bag.
#' @param data A MilData object, potentially generated by MilData().
#' @return list of 3:
#'     'useful_inst_names' includes the names of instances that were selected.
#'     'useful_inst_idx' includes the index of instances that were selected, at the instance level
#'     'yy' includes the bag labels, at the instance level
#' @examples
#' x = MilData(data.frame('bag_label' = factor(c(1, 1, 0)),
#'                        'bag_name' = c(rep('bag_1', 2), 'bag_2'),
#'                        instance_name' = c('bag_1_inst_1', 'bag_1_inst_2', 'bag_2_inst_1'),
#'                        'X1' = c(-0.4, 0.5, 2),
#'                        'instance_label' = c(0, 1, 0)))
#'
#' initialize_instance_selection(x)
#'
#' @export
#' @author Sean Kent
initialize_instance_selection <- function(data) {
    s_bag <- split(data, factor(data$bag_name, levels = unique(data$bag_name)))
    unique_bag_label <- function(x) { unique(x$bag_label) }
    unique_instance_name <- function(x) { unique(x$instance_name) }
    select_useful_inst <- function(label, name) { if (label == 1) {name[1]} else {name}}

    labels <- lapply(s_bag, FUN = unique_bag_label)
    instance_names <- lapply(s_bag, FUN = unique_instance_name)
    useful_inst_names <- unlist(mapply(FUN = select_useful_inst,
                                       label = labels,
                                       name = instance_names),
                                use.names = FALSE)
    useful_inst_idx <- which(unique(data$instance_name) %in% useful_inst_names)

    s_inst <- split(data, factor(data$instance_name, levels = unique(data$instance_name)))
    yy <-  unlist(lapply(s_inst, FUN = unique_bag_label),
                   use.names = FALSE)
    yy <- factor(yy, levels = c(0, 1), labels = c("0", "1"))  ## change yy to a factor.

    return(list(useful_inst_names = useful_inst_names,
                useful_inst_idx = useful_inst_idx,
                yy = yy))
}



##' Function to implement the iterative multiple instance learning with distributional data algorithm.
##'
##' Workhorse for the package. This implements the algorithm that
##' iteratively find the representative positive instances and use smm
##' to get the model.
##' @param data A MilData object, potentially generated by MilData().
##' @param cost The cost for smm.
##' @param max.step The total number of iterations.
##' @param sigma The parameter for the rbf kernel.
##' @param ... One can pass `kernel_full`, the Gram matrix at instance level for fast computation.
##' @return A mild object which contains the results.
##' @examples
##' MilData1 <- GenerateMilData(positive_dist = 'mvt',
##'                             negative_dist = 'mvnormal',
##'                             remainder_dist = 'mvnormal',
##'                             nbag = 10,
##'                             positive_degree = 3
##'                            )
##' foo <- mil_distribution(data = MilData1, cost = 1) ## uses about 10 seconds.
##' @export
##' @author Yifei Liu
mil_distribution <- function(data, cost, max.step = 500, sigma = 0.05, ...) {
    ## data should be of a MilData object.  bag_label should be one of '0'
    ## and '1', where '0' is negative bags and '1' is positive bags

    ## divide the bags to positive bags and negative bags

    bag_name <- data$bag_name
    bag_label <- data$bag_label
    instance_name <- unique(data$instance_name)
    if (length(unique(bag_label)) == 1)
        stop("Only one class label, cannot perform classification!")

    positive_bag_name <- unique(bag_name[bag_label == 1])
    negative_bag_name <- unique(bag_name[bag_label == 0])
    unique_bag_name <- unique(bag_name)
    n_bag <- length(unique_bag_name)  ## total number of bags

    ## Calculate the full kernel matrix, kernel_full is a
    ## length(instance_name) by length(instance_name) matrix.
    if (is.null(list(...)$kernel_full)) {
        kernel_full <- kme(df = data, sigma = sigma)
    } else {
        kernel_full <- list(...)$kernel_full
    }

    ## initialize the feature
    instance_selection <- initialize_instance_selection(data)
    useful_inst_idx = instance_selection[["useful_inst_idx"]]
    yy = instance_selection[["yy"]]

    num_neg_inst <- length(useful_inst_idx) - length(positive_bag_name)  ## calculate the number of negative instances.
    weights <- c(length(positive_bag_name)/num_neg_inst, 1)  ## this is less affected by the total number of bags.
    names(weights) <- c("0", "1")

    # ## iterate between updating the model and selecting the most positive
    # ## bag from an instance.
    # selection <- rep(0, length(positive_bag_name))  ## this records which instance is selected in which bag
    # past_selection <- matrix(NA, length(positive_bag_name), max.step)  ## this is the history of past selection.
    # past_selection[, 1] <- selection
    # ## step <- 1

    data_info <- unique(data[, c("bag_label", "bag_name", "instance_name")])
    temp_res <- kernel_mil(kernel_full, data_info, max.step, cost, weights,
        sigma, yy, useful_inst_idx)

    sample_df <- data[data$instance_name %in% instance_name[temp_res$useful_inst_idx],
        -c(1, 2)]
    res <- new_mild(list(model = temp_res$svm_model, total_step = temp_res$step,
        representative_inst = cbind(positive_bag_name, temp_res$selection),
        traindata = sample_df, useful_inst_idx = temp_res$useful_inst_idx))
    return(res)
}



##' Prediction function for mild objects
##'
##' Prediction function for mild objects.
##' @param object A mild object
##' @param ... should pass 'newdata', the new dataset to be predict on, optionally 'GramMatrix' to accelarate the speed.
##' @return A data.frame with the same number of rows as in newdata and with four columns, 'bag_name', 'instance_name', 'bag_label_pred', 'instance_score'
##' @examples
##' MilData1 <- GenerateMilData(positive_dist = 'mvt',
##'                             negative_dist = 'mvnormal',
##'                             remainder_dist = 'mvnormal',
##'                             nbag = 10,
##'                             positive_degree = 3,
##'                             nsample = 20
##' )
##' foo <- mil_distribution(data = MilData1, cost = 1) ## uses about 10 seconds.
##' predictions_mild <- predict(object = foo, newdata = MilData1)
##' @export
##' @importFrom stats predict
##' @importFrom pROC auc roc
##' @author Yifei Liu
predict.mild <- function(object, ...) {
    # newdata){
    args <- list(...)
    newdata = args$newdata
    ## auc = list(...)$newdata if(!is.null(auc) & auc &
    ## is.null(newdata$bag_label)) stop('newdata$bag_label should exist
    ## for AUC!')

    ## newdata should have form bag_label | bag_name | instance_name |
    ## feature_1 | ...

    ## traindata will come from mdl$traindata with the following form
    ## instance_name | feature_1 | ...

    ## label_pred <- predict(object = mdl$object, newx = newdata[,
    ## -(1:3)]) instance_label has the same length as the number of
    ## instances

    if (is.null(newdata$bag_label)) {
        instance_score <- predict(object = object$model,
                                  newdata = newdata %>%
                                      select(-bag_name),
                                  traindata = object$traindata,
                                  kernel_mild = object$kernel_mild)

        labels <- as.data.frame(cbind(newdata %>% dplyr::group_by(instance_name) %>%
            dplyr::summarise(bag_name = bag_name[1]), instance_score))

        bag_label <- labels %>% dplyr::group_by(bag_name) %>% dplyr::summarise(bag_label_pred = any(instance_score >
            0))
        final_pred <- newdata %>% dplyr::select(bag_name, instance_name) %>%
            dplyr::left_join(bag_label, by = "bag_name") %>% dplyr::left_join(labels %>%
            dplyr::mutate(instance_label = instance_score > 0) %>% dplyr::select(instance_name,
            instance_score), by = "instance_name")
        return(final_pred)
    }

    if (is.null(args$GramMatrix)) {
        instance_score <- predict(object = object$model,
                                  newdata = newdata %>%
                                      dplyr::select(-bag_name, -bag_label),
                                  traindata = object$traindata,
                                  kernel_mild = object$kernel_mild)
    } else {
        instance_score <- predict(object = object$model, kernel_mild = args$GramMatrix[object$useful_inst_idx,])
    }

    labels <- as.data.frame(cbind(unique(newdata[, c("bag_name", "instance_name",
        "bag_label")]), instance_score))

    labels_bag_level <- labels %>%
        dplyr::group_by(bag_name) %>%
        dplyr::summarise(bag_label_pred = any(instance_score > 0),
                         bag_score = max(instance_score),
                         bag_label = bag_label[1])
    final_pred <- newdata %>%
        dplyr::select(bag_name, instance_name) %>%
        dplyr::left_join(labels_bag_level, by = "bag_name") %>%
        dplyr::left_join(labels %>%
                             dplyr::mutate(instance_label_pred = instance_score > 0) %>%
                             dplyr::select(instance_name,instance_score, instance_label_pred),
                         by = "instance_name")
    ## calculate ROC and AUC
    roc_res <- pROC::roc(response = labels_bag_level$bag_label, predictor = labels_bag_level$bag_score)
    auc_res <- pROC::auc(roc_res)
    return(list(final_pred = final_pred, AUC = auc_res, ROC = roc_res))
}

##' Function to perform cross validation for the cost used in mil_distribution() function.
##'
##' Function to perform cross validation for the cost used in the
##' mil_distribution() function. The calculation can be accelerated if
##' the full gram matrix is provided.
##' @param data The dataset as a MilData object.
##' @param n_fold The number of folds.
##' @param fold_id A user specified fold_id. Suggested not to specify
##'     this.
##' @param cost_seq The cost sequence, default to be 2^(-2:2).
##' @param max.step Same as in mil_distribution(), default to be 300
##' @param sigma Same as in mil_distribution()
##' @param ... One can pass `kernel_full`, the Gram matrix at instance level for fast computation.
##' @return A list containing the best model and the optimal cost
##'     parameter.
##' @examples
##' MilData1 <- GenerateMilData(positive_dist = 'mvt',
##'                             negative_dist = 'mvnormal',
##'                             remainder_dist = 'mvnormal',
##'                             nbag = 20,
##'                             positive_degree = 3,
##'                             nsample = 10
##' )
##' ### This takes about 20 minutes.
##' ### foo <- cv_mild(MilData1, n_fold = 2)
##' ###
##' @export
##' @author Yifei Liu
cv_mild <- function(data, n_fold, fold_id, cost_seq = 2^(-2:2), max.step = 300,
    sigma = 0.05, ...) {

    bag_info <- unique(data[, c("bag_label", "bag_name")])
    if (missing(fold_id)) {
        if (missing(n_fold))
            n_fold = 5

        positive_bag_idx <- which(bag_info$bag_label == 1)
        negative_bag_idx <- which(bag_info$bag_label == 0)
        positive_fold_id <- base::sample((1:length(positive_bag_idx))%%n_fold +
            1)
        negative_fold_id <- base::sample((1:length(negative_bag_idx))%%n_fold +
            1)

        bag_id <- numeric(nrow(bag_info))
        bag_id[positive_bag_idx] <- positive_fold_id
        bag_id[negative_bag_idx] <- negative_fold_id

        temp_data <- data.frame(bag_name = unique(data$bag_name),
                                bag_id = bag_id,
                                stringsAsFactors = FALSE) %>%
            dplyr::right_join(unique(data %>% dplyr::select(bag_name, instance_name)),
                              by = "bag_name")
        fold_id <- temp_data$bag_id  ## now fold_id is of length(unique(data$instance_name))
    } else {
        n_fold <- max(fold_id)
        if (!is.null(setdiff(fold_id, 1:n_fold)))
            stop("The argument fold_id has some 'holes'!")
    }
    ## data should be of a MilData object.  bag_label should be one of '0'
    ## and '1', where '0' is negative bags and '1' is positive bags

    ## divide the bags to positive bags and negative bags

    bag_name <- data$bag_name
    bag_label <- data$bag_label
    instance_name <- unique(data$instance_name)
    if (length(unique(bag_label)) == 1)
        stop("Only one class label, cannot perform classification!")

    positive_bag_name <- unique(bag_name[bag_label == 1])
    negative_bag_name <- unique(bag_name[bag_label == 0])
    unique_bag_name <- unique(bag_name)
    n_bag <- length(unique_bag_name)  ## total number of bags

    ## Calculate the full kernel matrix, kernel_full is a
    ## length(instance_name) by length(instance_name) matrix.
    if (is.null(list(...)$kernel_full)) {
        kernel_full <- kme(df = data, sigma = sigma)
    } else {
        kernel_full <- list(...)$kernel_full
    }


    ## initialize the feature
    useful_inst_idx <- NULL  ## records which instances are used in the iterations, length not known in advance. This is at instance level, hence useful_inst_idx can never be larger than length(instance_name).
    yy_inst <- NULL  ## Label at instance level. Always has the same length as useful_inst_idx.
    last_inst_idx <- 0  ## at the beginning of each loop, idx is at the end of the last instance in the previous bag.

    for (i in 1:n_bag) {
        ## loop over all unique bags.
        data_i <- data[bag_name == unique_bag_name[i], ]  ## find the i-th bag
        n_inst_i <- length(unique(data_i$instance_name))  ## total number of instances in i-th bag
        if (data_i$bag_label[1] == 0) {
            # if all instances in this bag has a negative label, then this is a
            # negative bag


            useful_inst_idx <- c(useful_inst_idx, (last_inst_idx + 1):(last_inst_idx +
                n_inst_i))  ## add instances in this bag to instance index
            yy_inst <- c(yy_inst, rep(0, n_inst_i))  ## all instances in this bag have negative label
            last_inst_idx <- last_inst_idx + n_inst_i  ## move last_inst_idx to the end of this bag.

        } else if (data_i$bag_label[1] == 1) {
            ## if the bag is a positive bag
            useful_inst_idx <- c(useful_inst_idx, last_inst_idx + 1)  ## initialize using the first instance in the positive bag.
            yy_inst <- c(yy_inst, rep(1, n_inst_i))
            last_inst_idx <- last_inst_idx + length(unique(data_i$instance_name))
        } else {
            stop(paste("Bag labeling is inconsitent for ", i, "-th bag, ",
                unique_bag_name[i]))
        }
    }
    yy_inst <- factor(yy_inst, levels = c(0, 1), labels = c("0", "1"))  ## change yy_inst to a factor.
    num_neg_inst <- length(useful_inst_idx) - length(positive_bag_name)  ## calculate the number of negative instances.
    weights <- c(length(positive_bag_name)/num_neg_inst, 1)  ## this is less affected by the total number of bags.
    names(weights) <- c("0", "1")

    ## iterate between updating the model and selecting the most positive
    ## bag from an instance.

    data_info <- unique(data[, c("bag_label", "bag_name", "instance_name")])


    AUCs <- numeric(length(cost_seq))
    for (C in 1:length(cost_seq)) {
        temp_auc <- 0
        for (i in 1:n_fold) {
            train_instance_id <- which(fold_id != i)
            valid_instance_id <- which(fold_id == i)
            train_useful_inst_idx <- match(base::intersect(train_instance_id,
                useful_inst_idx), train_instance_id)
            temp_mdl <- kernel_mil(kernel_full = kernel_full[train_instance_id, train_instance_id],
                                   data_info = data_info[train_instance_id, ],
                                   max.step = max.step,
                                   cost = cost_seq[C],
                                   weights = weights,
                                   sigma = sigma,
                                   yy = yy_inst[train_instance_id],
                                   useful_inst_idx = train_useful_inst_idx)

            true_valid_bag_label <- unique(data_info[valid_instance_id, 1:2])$bag_label
            predictions_i <- predict(object = temp_mdl$svm_model,
                                     kernel_mild = kernel_full[train_instance_id[temp_mdl$useful_inst_idx], valid_instance_id])

            data_valid_info <- data_info[valid_instance_id, ]
            data_valid_info$instance_score_pred <- predictions_i
            bag_score_pred <- (data_valid_info %>% dplyr::group_by(bag_name) %>%
                dplyr::summarise(bag_score_pred = max(instance_score_pred)))$bag_score_pred
            temp_auc <- temp_auc + pROC::auc(pROC::roc(response = true_valid_bag_label,
                predictor = bag_score_pred))
        }
        AUCs[C] <- temp_auc/n_fold
    }

    bestC <- cost_seq[which.max(AUCs)]
    temp_res <- kernel_mil(kernel_full = kernel_full, data_info = data_info,
        max.step = max.step, cost = bestC, weights = weights, sigma = sigma,
        yy = yy_inst, useful_inst_idx = useful_inst_idx)

    sample_df <- data[data$instance_name %in% instance_name[temp_res$useful_inst_idx], -c(1, 2)]
    res <- new_mild(list(model = temp_res$svm_model,
                         total_step = temp_res$step,
                         representative_inst = cbind(positive_bag_name, temp_res$selection),
                         traindata = sample_df,
                         useful_inst_idx = useful_inst_idx))

    return(list(BestMdl = res, BestC = bestC, AUCs = AUCs, cost_seq = cost_seq))
}

##' Function to perform cross validation for the cost used in mil_distribution() function.
##'
##' Function to perform cross validation for the cost used in the mil_distribution() function.
##' @param data The dataset as a MilData object.
##' @param n_fold The number of folds.
##' @param fold_id A user specified fold_id. Suggested not to specify this.
##' @param cost_seq The cost sequence, default to be 2^(-2:2).
##' @param max.step Same as in mil_distribution(), default to be 300
##' @param sigma Same as in mil_distribution()
##' @return A list containing the best model and the optimal cost parameter.
##' @examples
##' MilData1 <- GenerateMilData(positive_dist = 'mvt',
##'                             negative_dist = 'mvnormal',
##'                             remainder_dist = 'mvnormal',
##'                             nbag = 20,
##'                             positive_degree = 3,
##'                             nsample = 10
##' )
##' ### This takes about 20 minutes.
##' ### foo <- cv_mild(MilData1, n_fold = 2)
##' ###
##'
##' @author Yifei Liu
cv_mild_old <- function(data, n_fold, fold_id, cost_seq = 2^(-2:2), max.step = 300,
    sigma = 0.05) {

    bag_info <- unique(data[, c("bag_label", "bag_name")])
    if (missing(fold_id)) {
        if (missing(n_fold))
            n_fold = 5

        positive_bag_idx <- which(bag_info$bag_label == 1)
        negative_bag_idx <- which(bag_info$bag_label == 0)
        positive_fold_id <- base::sample((1:length(positive_bag_idx))%%n_fold +
            1)
        negative_fold_id <- base::sample((1:length(negative_bag_idx))%%n_fold +
            1)

        bag_id <- numeric(nrow(bag_info))
        bag_id[positive_bag_idx] <- positive_fold_id
        bag_id[negative_bag_idx] <- negative_fold_id

        temp_data <- data.frame(bag_name = unique(data$bag_name), bag_id = bag_id,
            stringsAsFactors = FALSE) %>% dplyr::right_join(unique(data %>%
            dplyr::select(bag_name, instance_name)), by = "bag_name")
        fold_id <- temp_data$bag_id  ## now fold_id is of length(unique(data$instance_name))
    } else {
        n_fold <- max(fold_id)
        if (!is.null(setdiff(fold_id, 1:n_fold)))
            stop("The argument fold_id has some 'holes'!")
    }
    ## data should be of a MilData object.  bag_label should be one of '0'
    ## and '1', where '0' is negative bags and '1' is positive bags

    ## divide the bags to positive bags and negative bags

    bag_name <- data$bag_name
    bag_label <- data$bag_label
    instance_name <- unique(data$instance_name)
    if (length(unique(bag_label)) == 1)
        stop("Only one class label, cannot perform classification!")

    positive_bag_name <- unique(bag_name[bag_label == 1])
    negative_bag_name <- unique(bag_name[bag_label == 0])
    unique_bag_name <- unique(bag_name)
    n_bag <- length(unique_bag_name)  ## total number of bags

    ## Calculate the full kernel matrix, kernel_full is a
    ## length(instance_name) by length(instance_name) matrix.
    kernel_full <- kme(df = data, sigma = sigma)

    ## initialize the feature
    useful_inst_idx <- NULL  ## records which instances are used in the iterations, length not known in advance. This is at instance level, hence useful_inst_idx can never be larger than length(instance_name).
    yy_inst <- NULL  ## Label at instance level. Always has the same length as useful_inst_idx.
    last_inst_idx <- 0  ## at the beginning of each loop, idx is at the end of the last instance in the previous bag.

    for (i in 1:n_bag) {
        ## loop over all unique bags.
        data_i <- data[bag_name == unique_bag_name[i], ]  ## find the i-th bag
        n_inst_i <- length(unique(data_i$instance_name))  ## total number of instances in i-th bag
        if (data_i$bag_label[1] == 0) {
            # if all instances in this bag has a negative label, then this is a
            # negative bag


            useful_inst_idx <- c(useful_inst_idx, (last_inst_idx + 1):(last_inst_idx +
                n_inst_i))  ## add instances in this bag to instance index
            yy_inst <- c(yy_inst, rep(0, n_inst_i))  ## all instances in this bag have negative label
            last_inst_idx <- last_inst_idx + n_inst_i  ## move last_inst_idx to the end of this bag.

        } else if (data_i$bag_label[1] == 1) {
            ## if the bag is a positive bag
            useful_inst_idx <- c(useful_inst_idx, last_inst_idx + 1)  ## initialize using the first instance in the positive bag.
            yy_inst <- c(yy_inst, rep(1, n_inst_i))
            last_inst_idx <- last_inst_idx + length(unique(data_i$instance_name))
        } else {
            stop(paste("Bag labeling is inconsitent for ", i, "-th bag, ",
                unique_bag_name[i]))
        }
    }
    yy_inst <- factor(yy_inst, levels = c(0, 1), labels = c("0", "1"))  ## change yy_inst to a factor.
    num_neg_inst <- length(useful_inst_idx) - length(positive_bag_name)  ## calculate the number of negative instances.
    weights <- c(length(positive_bag_name)/num_neg_inst, 1)  ## this is less affected by the total number of bags.
    names(weights) <- c("0", "1")

    ## iterate between updating the model and selecting the most positive
    ## bag from an instance.

    data_info <- unique(data[, c("bag_label", "bag_name", "instance_name")])


    AUCs <- numeric(length(cost_seq))
    for (C in 1:length(cost_seq)) {
        temp_auc <- 0
        for (i in 1:n_fold) {
            train_instance_id <- which(fold_id != i)
            valid_instance_id <- which(fold_id == i)
            train_useful_inst_idx <- match(base::intersect(train_instance_id,
                useful_inst_idx), train_instance_id)
            temp_mdl <- kernel_mil(kernel_full = kernel_full[train_instance_id,
                train_instance_id], data_info = data_info[train_instance_id,
                ], max.step = max.step, cost = cost_seq[C], weights = weights,
                sigma = sigma, yy = yy_inst[train_instance_id], useful_inst_idx = train_useful_inst_idx)

            true_valid_bag_label <- unique(data_info[valid_instance_id,
                1:2])$bag_label
            predictions_i <- predict(object = temp_mdl$svm_model, kernel_mild = kernel_full[train_instance_id[temp_mdl$useful_inst_idx],
                valid_instance_id])

            data_valid_info <- data_info[valid_instance_id, ]
            data_valid_info$instance_score_pred <- predictions_i
            bag_score_pred <- (data_valid_info %>% dplyr::group_by(bag_name) %>%
                dplyr::summarise(bag_score_pred = max(instance_score_pred)))$bag_score_pred
            temp_auc <- temp_auc + pROC::auc(pROC::roc(response = true_valid_bag_label,
                predictor = bag_score_pred))
        }
        AUCs[C] <- temp_auc/n_fold
    }

    bestC <- cost_seq[which.max(AUCs)]
    temp_res <- kernel_mil(kernel_full = kernel_full, data_info = data_info,
        max.step = max.step, cost = bestC, weights = weights, sigma = sigma,
        yy = yy_inst, useful_inst_idx = useful_inst_idx)

    sample_df <- data[data$instance_name %in% instance_name[temp_res$useful_inst_idx],
        -c(1, 2)]
    res <- new_mild(list(model = temp_res$svm_model, total_step = temp_res$step,
        representative_inst = cbind(positive_bag_name, temp_res$selection),
        traindata = sample_df))

    return(list(BestMdl = res, BestC = bestC, AUCs = AUCs, cost_seq = cost_seq))
}
