# `build_fm()`, `kfm_exact()`, `kfm_nystrom()` examples work

    Code
      df <- data.frame(X1 = c(2, 3, 4, 5, 6, 7, 8), X2 = c(1, 1.2, 1.3, 1.4, 1.1, 7,
        1), X3 = rnorm(7))
      fit1 <- kfm_nystrom(df, m = 7, r = 6, kernel = "radial", sigma = 0.05)
      fm <- build_fm(fit1, df)
      fit2 <- kfm_exact(kernel = "polynomial", degree = 2, const = 1)
      fm <- build_fm(fit2, df)

# `cv_misvm()` examples work

    Code
      set.seed(8)
      mil_data <- generate_mild_df(nbag = 20, positive_prob = 0.15, dist = rep(
        "mvnormal", 3), mean = list(rep(1, 10), rep(2, 10)), sd_of_mean = rep(0.1, 3))
      df <- build_instance_feature(mil_data, seq(0.05, 0.95, length.out = 10))
      cost_seq <- 2^seq(-5, 7, length.out = 3)
      mdl1 <- cv_misvm(x = df[, 4:123], y = df$bag_label, bags = df$bag_name,
      cost_seq = cost_seq, n_fold = 3, method = "heuristic")
      mdl2 <- cv_misvm(mi(bag_label, bag_name) ~ X1_mean + X2_mean + X3_mean, data = df,
      cost_seq = cost_seq, n_fold = 3)
      if (require(gurobi)) {
        mdl3 <- cv_misvm(x = df[, 4:123], y = df$bag_label, bags = df$bag_name,
        cost_seq = cost_seq, n_fold = 3, method = "mip")
      }
    Message <packageStartupMessage>
      Loading required package: gurobi
      Loading required package: slam
    Code
      predict(mdl1, new_data = df, type = "raw", layer = "bag")
    Output
      # A tibble: 80 x 1
         .pred
         <dbl>
       1 -1.00
       2 -1.00
       3 -1.00
       4 -1.00
       5  1.04
       6  1.04
       7  1.04
       8  1.04
       9 -1.13
      10 -1.13
      # ... with 70 more rows
    Code
      df %>% bind_cols(predict(mdl2, df, type = "class")) %>% bind_cols(predict(mdl2,
        df, type = "raw")) %>% distinct(bag_name, bag_label, .pred_class, .pred)
    Output
         bag_label bag_name .pred_class      .pred
      1          0     bag1           0 -0.5932349
      2          1     bag2           1  0.7493612
      3          0     bag3           0 -0.9387030
      4          1     bag4           1  1.2126533
      5          0     bag5           0 -0.8094506
      6          0     bag6           0 -0.8083522
      7          1     bag7           1  0.6587946
      8          0     bag8           0 -0.9032079
      9          1     bag9           1  0.5855234
      10         1    bag10           1  1.2019300
      11         1    bag11           1  1.2689043
      12         0    bag12           0 -0.8143970
      13         1    bag13           1  0.8591738
      14         1    bag14           1  1.0000000
      15         1    bag15           1  1.1078369
      16         1    bag16           1  1.2117319
      17         0    bag17           0 -0.6022075
      18         1    bag18           1  0.9355648
      19         0    bag19           0 -0.7314129
      20         1    bag20           1  1.0393764

# `generate_mild_df()` examples work

    Code
      set.seed(8)
      mild_data <- generate_mild_df(nbag = 7, ninst = 3, nsample = 20, ncov = 2,
        nimp_pos = 1, dist = rep("mvnormal", 3), mean = list(rep(5, 1), rep(15, 2), 0))
      library(dplyr)
      distinct(mild_data, bag_label, bag_name, instance_name)
    Output
         bag_label bag_name instance_name
      1          0     bag1     bag1inst1
      2          0     bag1     bag1inst2
      3          0     bag1     bag1inst3
      4          0     bag2     bag2inst1
      5          0     bag2     bag2inst2
      6          0     bag2     bag2inst3
      7          1     bag3     bag3inst1
      8          1     bag3     bag3inst2
      9          1     bag3     bag3inst3
      10         0     bag4     bag4inst1
      11         0     bag4     bag4inst2
      12         0     bag4     bag4inst3
      13         0     bag5     bag5inst1
      14         0     bag5     bag5inst2
      15         0     bag5     bag5inst3
      16         1     bag6     bag6inst1
      17         1     bag6     bag6inst2
      18         1     bag6     bag6inst3
      19         0     bag7     bag7inst1
      20         0     bag7     bag7inst2
      21         0     bag7     bag7inst3
    Code
      split(mild_data[, 4:5], mild_data$instance_name) %>% sapply(colMeans) %>% round(
        2) %>% t()
    Output
                   X1    X2
      bag1inst1 14.95 14.63
      bag1inst2 15.53 15.42
      bag1inst3 14.27 16.08
      bag2inst1 15.29 15.01
      bag2inst2 15.78 14.87
      bag2inst3 14.53 15.71
      bag3inst1 14.79 14.83
      bag3inst2  5.36 -0.69
      bag3inst3 15.13 15.78
      bag4inst1 15.32 14.46
      bag4inst2 15.31 15.53
      bag4inst3 15.93 14.55
      bag5inst1 15.25 14.25
      bag5inst2 14.98 16.03
      bag5inst3 15.48 14.95
      bag6inst1  4.83  0.26
      bag6inst2 16.23 16.06
      bag6inst3 15.31 14.56
      bag7inst1 14.74 14.90
      bag7inst2 14.61 15.03
      bag7inst3 15.17 15.20

# `kme()` examples work

    Code
      x = data.frame(instance_name = c("inst_1", "inst_2", "inst_1"), X1 = c(-0.4,
        0.5, 2))
      kme(x)
    Output
                [,1]      [,2]
      [1,] 0.8748808 0.9269533
      [2,] 0.9269533 1.0000000
    Code
      mild_df1 <- generate_mild_df(nbag = 10, positive_degree = 3)
      kme(mild_df1)
    Output
                 [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]
       [1,] 0.4169122 0.3523994 0.2999620 0.3919451 0.2671537 0.3808291 0.3178319
       [2,] 0.3523994 0.4001522 0.3488563 0.3862459 0.3258604 0.3828253 0.3553224
       [3,] 0.2999620 0.3488563 0.3830298 0.3729092 0.3410245 0.3829949 0.3563669
       [4,] 0.3919451 0.3862459 0.3729092 0.6736729 0.3829977 0.4622801 0.3786055
       [5,] 0.2671537 0.3258604 0.3410245 0.3829977 0.3866819 0.3438624 0.3125532
       [6,] 0.3808291 0.3828253 0.3829949 0.4622801 0.3438624 0.5928032 0.3992530
       [7,] 0.3178319 0.3553224 0.3563669 0.3786055 0.3125532 0.3992530 0.4072973
       [8,] 0.2886348 0.3157459 0.2987742 0.3711378 0.2682204 0.3401411 0.2749737
       [9,] 0.3872634 0.4011600 0.3740848 0.4325844 0.3109014 0.4722626 0.3594980
      [10,] 0.3086363 0.3301951 0.2900993 0.3880930 0.2750935 0.3674942 0.3147702
      [11,] 0.3300471 0.3715490 0.3490256 0.3765055 0.3468922 0.3911776 0.3134912
      [12,] 0.3496296 0.3733415 0.3661626 0.5378319 0.3513647 0.4593275 0.3566796
      [13,] 0.2643695 0.3083248 0.3592159 0.3650698 0.3075189 0.3443388 0.3521353
      [14,] 0.3510899 0.3496693 0.3579273 0.4438735 0.3136538 0.4244626 0.3410867
      [15,] 0.3306274 0.3505787 0.3460770 0.4034540 0.3067107 0.4154028 0.3552658
      [16,] 0.3547158 0.3407101 0.3166309 0.3790609 0.2549436 0.3913973 0.3093184
      [17,] 0.3413221 0.3310640 0.3056185 0.4769570 0.2984654 0.3205515 0.3010842
      [18,] 0.3385712 0.3715150 0.3362378 0.3772033 0.3236153 0.3631170 0.3673017
      [19,] 0.3274643 0.3157874 0.3209078 0.3923291 0.2950952 0.3912085 0.3039875
      [20,] 0.3024896 0.3319636 0.3544274 0.3987057 0.3375483 0.4542176 0.3624589
      [21,] 0.3037886 0.3305133 0.3058440 0.4023323 0.2933122 0.3999505 0.3038551
      [22,] 0.4046782 0.4063992 0.3687206 0.4748479 0.3561605 0.5333569 0.3857608
      [23,] 0.2921442 0.3529623 0.2947198 0.3825395 0.2826722 0.3598854 0.2964741
      [24,] 0.3670381 0.3722187 0.3556825 0.4504677 0.3279440 0.4540149 0.3728457
      [25,] 0.3293533 0.3842095 0.3365811 0.3505797 0.3083936 0.3654656 0.3492568
      [26,] 0.4514919 0.4317108 0.3776675 0.5362912 0.3806510 0.4668916 0.3974679
      [27,] 0.3072870 0.3522197 0.3407901 0.4046125 0.3505049 0.3589900 0.3183809
      [28,] 0.3351851 0.3453512 0.3313021 0.4664949 0.3088645 0.4280774 0.3465275
      [29,] 0.4277095 0.3869842 0.3158980 0.4345843 0.2831570 0.3771593 0.3238047
      [30,] 0.3918940 0.4233512 0.3803228 0.5079662 0.3517907 0.4494309 0.3917467
      [31,] 0.3209992 0.3356011 0.3067535 0.3937972 0.2606738 0.3971114 0.3341408
      [32,] 0.3304344 0.3615479 0.3098770 0.3813640 0.3323581 0.3808892 0.3229543
      [33,] 0.3633943 0.3933628 0.4126571 0.4078071 0.3401322 0.4569400 0.3942821
      [34,] 0.3187196 0.3354348 0.3444837 0.4187835 0.3351518 0.3974189 0.3215929
      [35,] 0.3067201 0.3295164 0.3086434 0.4358080 0.2989232 0.3744900 0.3193445
      [36,] 0.3709173 0.3094088 0.2352705 0.3380354 0.2257843 0.3056635 0.2402585
      [37,] 0.3747756 0.3482068 0.3025652 0.4241205 0.2659261 0.4055969 0.3187351
      [38,] 0.3645088 0.3434428 0.3443278 0.3946202 0.2756704 0.4240096 0.3299400
      [39,] 0.3080024 0.3640326 0.3355290 0.3848462 0.2924023 0.3766813 0.3508378
      [40,] 0.4362794 0.4125146 0.3767652 0.4305746 0.3363838 0.4993309 0.3431162
                 [,8]      [,9]     [,10]     [,11]     [,12]     [,13]     [,14]
       [1,] 0.2886348 0.3872634 0.3086363 0.3300471 0.3496296 0.2643695 0.3510899
       [2,] 0.3157459 0.4011600 0.3301951 0.3715490 0.3733415 0.3083248 0.3496693
       [3,] 0.2987742 0.3740848 0.2900993 0.3490256 0.3661626 0.3592159 0.3579273
       [4,] 0.3711378 0.4325844 0.3880930 0.3765055 0.5378319 0.3650698 0.4438735
       [5,] 0.2682204 0.3109014 0.2750935 0.3468922 0.3513647 0.3075189 0.3136538
       [6,] 0.3401411 0.4722626 0.3674942 0.3911776 0.4593275 0.3443388 0.4244626
       [7,] 0.2749737 0.3594980 0.3147702 0.3134912 0.3566796 0.3521353 0.3410867
       [8,] 0.3827260 0.3907545 0.2988163 0.3308321 0.3751270 0.2927895 0.3424322
       [9,] 0.3907545 0.5245163 0.3503800 0.3956145 0.4558087 0.3169918 0.4143504
      [10,] 0.2988163 0.3503800 0.3778166 0.3133673 0.3854424 0.2845932 0.3253879
      [11,] 0.3308321 0.3956145 0.3133673 0.4192976 0.3827056 0.3105342 0.3439707
      [12,] 0.3751270 0.4558087 0.3854424 0.3827056 0.5381580 0.3410197 0.4040592
      [13,] 0.2927895 0.3169918 0.2845932 0.3105342 0.3410197 0.4152752 0.3384331
      [14,] 0.3424322 0.4143504 0.3253879 0.3439707 0.4040592 0.3384331 0.4227283
      [15,] 0.3529021 0.4052576 0.3318198 0.3378625 0.3899277 0.3435705 0.3792346
      [16,] 0.3247552 0.4200867 0.3237023 0.3466853 0.3903709 0.3004923 0.3533612
      [17,] 0.3023912 0.3428421 0.3114730 0.3110601 0.3819310 0.3000919 0.3629485
      [18,] 0.2923464 0.3394161 0.3164056 0.3531750 0.3375635 0.3346717 0.3247464
      [19,] 0.2713716 0.3836950 0.2482132 0.3223733 0.3761191 0.2686236 0.3305258
      [20,] 0.2857859 0.3744232 0.3174561 0.3395988 0.3984483 0.3315578 0.3550107
      [21,] 0.2627253 0.3836853 0.2795323 0.3301624 0.3831656 0.2488448 0.3148869
      [22,] 0.3406292 0.4621339 0.3923090 0.3955028 0.4495619 0.3141189 0.4283485
      [23,] 0.3157345 0.3856858 0.3494546 0.3538506 0.3917857 0.2697028 0.3082309
      [24,] 0.3275562 0.4251279 0.3382506 0.3577489 0.4262159 0.3256938 0.3830319
      [25,] 0.3231252 0.3904057 0.3485463 0.3771474 0.3788117 0.3159366 0.3161134
      [26,] 0.3373112 0.4363885 0.4001063 0.4178917 0.4635879 0.3431673 0.4212106
      [27,] 0.3388433 0.3562988 0.3225072 0.3660954 0.3718644 0.3297816 0.3608778
      [28,] 0.2935088 0.3908359 0.3247119 0.3233924 0.4104011 0.3010204 0.3750590
      [29,] 0.3255437 0.4056847 0.3709777 0.3798311 0.3901575 0.3028651 0.3793250
      [30,] 0.3773517 0.4505271 0.4349212 0.4046103 0.4724630 0.3729573 0.4305713
      [31,] 0.3151219 0.4048709 0.3019086 0.2964317 0.3670353 0.2776221 0.3516817
      [32,] 0.3101098 0.3605160 0.3237806 0.3617166 0.3625330 0.2743012 0.3224096
      [33,] 0.3271257 0.4471554 0.3090527 0.3839613 0.3851377 0.3774804 0.4184469
      [34,] 0.3434242 0.3851384 0.2983263 0.3592944 0.3967367 0.3314950 0.3647405
      [35,] 0.3355117 0.3918822 0.3211793 0.3102746 0.4325159 0.2788364 0.3399177
      [36,] 0.2370985 0.3357440 0.2703925 0.2874996 0.2988993 0.1794404 0.2905320
      [37,] 0.3102639 0.4125572 0.3560512 0.3232770 0.4074992 0.2703437 0.3675738
      [38,] 0.3191465 0.4189078 0.3061028 0.3536871 0.3836219 0.3285568 0.3830229
      [39,] 0.3091127 0.3884269 0.3297345 0.3310730 0.3640645 0.3208774 0.3476835
      [40,] 0.3742521 0.5217239 0.3386285 0.4284365 0.4327775 0.2919186 0.4346810
                [,15]     [,16]     [,17]     [,18]     [,19]     [,20]     [,21]
       [1,] 0.3306274 0.3547158 0.3413221 0.3385712 0.3274643 0.3024896 0.3037886
       [2,] 0.3505787 0.3407101 0.3310640 0.3715150 0.3157874 0.3319636 0.3305133
       [3,] 0.3460770 0.3166309 0.3056185 0.3362378 0.3209078 0.3544274 0.3058440
       [4,] 0.4034540 0.3790609 0.4769570 0.3772033 0.3923291 0.3987057 0.4023323
       [5,] 0.3067107 0.2549436 0.2984654 0.3236153 0.2950952 0.3375483 0.2933122
       [6,] 0.4154028 0.3913973 0.3205515 0.3631170 0.3912085 0.4542176 0.3999505
       [7,] 0.3552658 0.3093184 0.3010842 0.3673017 0.3039875 0.3624589 0.3038551
       [8,] 0.3529021 0.3247552 0.3023912 0.2923464 0.2713716 0.2857859 0.2627253
       [9,] 0.4052576 0.4200867 0.3428421 0.3394161 0.3836950 0.3744232 0.3836853
      [10,] 0.3318198 0.3237023 0.3114730 0.3164056 0.2482132 0.3174561 0.2795323
      [11,] 0.3378625 0.3466853 0.3110601 0.3531750 0.3223733 0.3395988 0.3301624
      [12,] 0.3899277 0.3903709 0.3819310 0.3375635 0.3761191 0.3984483 0.3831656
      [13,] 0.3435705 0.3004923 0.3000919 0.3346717 0.2686236 0.3315578 0.2488448
      [14,] 0.3792346 0.3533612 0.3629485 0.3247464 0.3305258 0.3550107 0.3148869
      [15,] 0.4029821 0.3329319 0.3200534 0.3483374 0.3092085 0.3560920 0.2891205
      [16,] 0.3329319 0.4072145 0.3136260 0.3063168 0.3153526 0.3135427 0.3179032
      [17,] 0.3200534 0.3136260 0.4178473 0.3216383 0.2921914 0.2881410 0.2887689
      [18,] 0.3483374 0.3063168 0.3216383 0.4130369 0.2840406 0.3254596 0.2799843
      [19,] 0.3092085 0.3153526 0.2921914 0.2840406 0.3959299 0.3322660 0.3393178
      [20,] 0.3560920 0.3135427 0.2881410 0.3254596 0.3322660 0.4044005 0.3257537
      [21,] 0.2891205 0.3179032 0.2887689 0.2799843 0.3393178 0.3257537 0.4047794
      [22,] 0.4091661 0.3738722 0.3595939 0.3721547 0.3633162 0.4213864 0.3880263
      [23,] 0.3054019 0.3466469 0.3022233 0.3044542 0.2604175 0.2995181 0.3488779
      [24,] 0.3812947 0.3548821 0.3396105 0.3582028 0.3601843 0.3795341 0.3540605
      [25,] 0.3478419 0.3491553 0.2969050 0.3747642 0.2905926 0.3273992 0.3005663
      [26,] 0.3904225 0.3992663 0.4359671 0.4253176 0.3837960 0.3945064 0.3896817
      [27,] 0.3585833 0.3014212 0.3401771 0.3509750 0.2805411 0.3273872 0.2782515
      [28,] 0.3428719 0.3314276 0.3467582 0.3160977 0.3306523 0.3545179 0.3646012
      [29,] 0.3394214 0.4170320 0.3993973 0.3675633 0.3038418 0.3064804 0.3261697
      [30,] 0.4099238 0.4139901 0.4213449 0.3990849 0.3207753 0.3835597 0.3628194
      [31,] 0.3567115 0.3172887 0.3049912 0.3059889 0.3020181 0.3197710 0.3215577
      [32,] 0.3483399 0.2912989 0.3062592 0.3634603 0.2945235 0.3313570 0.2940884
      [33,] 0.3816499 0.3762535 0.3381053 0.3651828 0.3660556 0.3826280 0.3621237
      [34,] 0.3678903 0.3200297 0.3223298 0.3341906 0.3314275 0.3497056 0.2945803
      [35,] 0.3583229 0.3023115 0.3210037 0.3041208 0.3219870 0.3326105 0.3030481
      [36,] 0.2544942 0.3001889 0.3109006 0.2631206 0.2796083 0.2363907 0.2927193
      [37,] 0.3423002 0.3730642 0.3483797 0.3060860 0.3134191 0.3240729 0.3292184
      [38,] 0.3477659 0.3961409 0.3230067 0.3213942 0.3424865 0.3367813 0.3246403
      [39,] 0.3423561 0.3343108 0.3173556 0.3336023 0.2765495 0.3222561 0.3275091
      [40,] 0.3966873 0.4149241 0.3552640 0.3474666 0.4134704 0.3821023 0.3961611
                [,22]     [,23]     [,24]     [,25]     [,26]     [,27]     [,28]
       [1,] 0.4046782 0.2921442 0.3670381 0.3293533 0.4514919 0.3072870 0.3351851
       [2,] 0.4063992 0.3529623 0.3722187 0.3842095 0.4317108 0.3522197 0.3453512
       [3,] 0.3687206 0.2947198 0.3556825 0.3365811 0.3776675 0.3407901 0.3313021
       [4,] 0.4748479 0.3825395 0.4504677 0.3505797 0.5362912 0.4046125 0.4664949
       [5,] 0.3561605 0.2826722 0.3279440 0.3083936 0.3806510 0.3505049 0.3088645
       [6,] 0.5333569 0.3598854 0.4540149 0.3654656 0.4668916 0.3589900 0.4280774
       [7,] 0.3857608 0.2964741 0.3728457 0.3492568 0.3974679 0.3183809 0.3465275
       [8,] 0.3406292 0.3157345 0.3275562 0.3231252 0.3373112 0.3388433 0.2935088
       [9,] 0.4621339 0.3856858 0.4251279 0.3904057 0.4363885 0.3562988 0.3908359
      [10,] 0.3923090 0.3494546 0.3382506 0.3485463 0.4001063 0.3225072 0.3247119
      [11,] 0.3955028 0.3538506 0.3577489 0.3771474 0.4178917 0.3660954 0.3233924
      [12,] 0.4495619 0.3917857 0.4262159 0.3788117 0.4635879 0.3718644 0.4104011
      [13,] 0.3141189 0.2697028 0.3256938 0.3159366 0.3431673 0.3297816 0.3010204
      [14,] 0.4283485 0.3082309 0.3830319 0.3161134 0.4212106 0.3608778 0.3750590
      [15,] 0.4091661 0.3054019 0.3812947 0.3478419 0.3904225 0.3585833 0.3428719
      [16,] 0.3738722 0.3466469 0.3548821 0.3491553 0.3992663 0.3014212 0.3314276
      [17,] 0.3595939 0.3022233 0.3396105 0.2969050 0.4359671 0.3401771 0.3467582
      [18,] 0.3721547 0.3044542 0.3582028 0.3747642 0.4253176 0.3509750 0.3160977
      [19,] 0.3633162 0.2604175 0.3601843 0.2905926 0.3837960 0.2805411 0.3306523
      [20,] 0.4213864 0.2995181 0.3795341 0.3273992 0.3945064 0.3273872 0.3545179
      [21,] 0.3880263 0.3488779 0.3540605 0.3005663 0.3896817 0.2782515 0.3646012
      [22,] 0.5630475 0.3753065 0.4449969 0.3733252 0.5032117 0.3874373 0.4322735
      [23,] 0.3753065 0.4471016 0.3311823 0.3651890 0.3853135 0.3199225 0.3380487
      [24,] 0.4449969 0.3311823 0.4151443 0.3564034 0.4466317 0.3462490 0.3836512
      [25,] 0.3733252 0.3651890 0.3564034 0.4298102 0.4069444 0.3413267 0.3100067
      [26,] 0.5032117 0.3853135 0.4466317 0.4069444 0.5818781 0.4032938 0.4299712
      [27,] 0.3874373 0.3199225 0.3462490 0.3413267 0.4032938 0.3995114 0.3234780
      [28,] 0.4322735 0.3380487 0.3836512 0.3100067 0.4299712 0.3234780 0.4043163
      [29,] 0.4160148 0.3796291 0.3704163 0.3798308 0.5021081 0.3503216 0.3583275
      [30,] 0.4824457 0.4395676 0.4209159 0.4195498 0.5101399 0.4143516 0.4201927
      [31,] 0.3968881 0.3119012 0.3654678 0.3098126 0.3696027 0.3010211 0.3550438
      [32,] 0.4147757 0.3143400 0.3609095 0.3576175 0.4206899 0.3590449 0.3179471
      [33,] 0.4358190 0.3327537 0.4046812 0.3552375 0.4318491 0.3590810 0.3871017
      [34,] 0.3851446 0.2917805 0.3678323 0.3276997 0.3922857 0.3619419 0.3271772
      [35,] 0.3826448 0.3047897 0.3715418 0.3309257 0.3824156 0.3248125 0.3375729
      [36,] 0.3627135 0.2818273 0.3044356 0.2729893 0.4067361 0.2578816 0.2968476
      [37,] 0.4269345 0.3464054 0.3738811 0.3354771 0.4384104 0.3105591 0.3670421
      [38,] 0.3923521 0.3149255 0.3712310 0.3314089 0.4082615 0.3153282 0.3489113
      [39,] 0.3864728 0.3746239 0.3510428 0.3499524 0.3858336 0.3302087 0.3543242
      [40,] 0.5149685 0.3622524 0.4349479 0.3730711 0.4803461 0.3754398 0.3987842
                [,29]     [,30]     [,31]     [,32]     [,33]     [,34]     [,35]
       [1,] 0.4277095 0.3918940 0.3209992 0.3304344 0.3633943 0.3187196 0.3067201
       [2,] 0.3869842 0.4233512 0.3356011 0.3615479 0.3933628 0.3354348 0.3295164
       [3,] 0.3158980 0.3803228 0.3067535 0.3098770 0.4126571 0.3444837 0.3086434
       [4,] 0.4345843 0.5079662 0.3937972 0.3813640 0.4078071 0.4187835 0.4358080
       [5,] 0.2831570 0.3517907 0.2606738 0.3323581 0.3401322 0.3351518 0.2989232
       [6,] 0.3771593 0.4494309 0.3971114 0.3808892 0.4569400 0.3974189 0.3744900
       [7,] 0.3238047 0.3917467 0.3341408 0.3229543 0.3942821 0.3215929 0.3193445
       [8,] 0.3255437 0.3773517 0.3151219 0.3101098 0.3271257 0.3434242 0.3355117
       [9,] 0.4056847 0.4505271 0.4048709 0.3605160 0.4471554 0.3851384 0.3918822
      [10,] 0.3709777 0.4349212 0.3019086 0.3237806 0.3090527 0.2983263 0.3211793
      [11,] 0.3798311 0.4046103 0.2964317 0.3617166 0.3839613 0.3592944 0.3102746
      [12,] 0.3901575 0.4724630 0.3670353 0.3625330 0.3851377 0.3967367 0.4325159
      [13,] 0.3028651 0.3729573 0.2776221 0.2743012 0.3774804 0.3314950 0.2788364
      [14,] 0.3793250 0.4305713 0.3516817 0.3224096 0.4184469 0.3647405 0.3399177
      [15,] 0.3394214 0.4099238 0.3567115 0.3483399 0.3816499 0.3678903 0.3583229
      [16,] 0.4170320 0.4139901 0.3172887 0.2912989 0.3762535 0.3200297 0.3023115
      [17,] 0.3993973 0.4213449 0.3049912 0.3062592 0.3381053 0.3223298 0.3210037
      [18,] 0.3675633 0.3990849 0.3059889 0.3634603 0.3651828 0.3341906 0.3041208
      [19,] 0.3038418 0.3207753 0.3020181 0.2945235 0.3660556 0.3314275 0.3219870
      [20,] 0.3064804 0.3835597 0.3197710 0.3313570 0.3826280 0.3497056 0.3326105
      [21,] 0.3261697 0.3628194 0.3215577 0.2940884 0.3621237 0.2945803 0.3030481
      [22,] 0.4160148 0.4824457 0.3968881 0.4147757 0.4358190 0.3851446 0.3826448
      [23,] 0.3796291 0.4395676 0.3119012 0.3143400 0.3327537 0.2917805 0.3047897
      [24,] 0.3704163 0.4209159 0.3654678 0.3609095 0.4046812 0.3678323 0.3715418
      [25,] 0.3798308 0.4195498 0.3098126 0.3576175 0.3552375 0.3276997 0.3309257
      [26,] 0.5021081 0.5101399 0.3696027 0.4206899 0.4318491 0.3922857 0.3824156
      [27,] 0.3503216 0.4143516 0.3010211 0.3590449 0.3590810 0.3619419 0.3248125
      [28,] 0.3583275 0.4201927 0.3550438 0.3179471 0.3871017 0.3271772 0.3375729
      [29,] 0.5562114 0.4896433 0.3201169 0.3412564 0.3858215 0.3268655 0.2998863
      [30,] 0.4896433 0.5604001 0.3818702 0.3887106 0.4317382 0.3790934 0.3781906
      [31,] 0.3201169 0.3818702 0.3813176 0.3107423 0.3666519 0.3145105 0.3413322
      [32,] 0.3412564 0.3887106 0.3107423 0.4028639 0.3279293 0.3447714 0.3375131
      [33,] 0.3858215 0.4317382 0.3666519 0.3279293 0.5115058 0.3683147 0.3183061
      [34,] 0.3268655 0.3790934 0.3145105 0.3447714 0.3683147 0.3886675 0.3474166
      [35,] 0.2998863 0.3781906 0.3413322 0.3375131 0.3183061 0.3474166 0.4157505
      [36,] 0.3992034 0.3401047 0.2711894 0.2902113 0.2902643 0.2515062 0.2591211
      [37,] 0.4197093 0.4377885 0.3444593 0.3202151 0.3562870 0.3131996 0.3377408
      [38,] 0.4048258 0.4059210 0.3258159 0.2927319 0.4211501 0.3429983 0.2983918
      [39,] 0.3608347 0.4316374 0.3462349 0.3132448 0.3930252 0.3068435 0.3068626
      [40,] 0.4386672 0.4426953 0.3904327 0.3934714 0.4677351 0.4012890 0.3686771
                [,36]     [,37]     [,38]     [,39]     [,40]
       [1,] 0.3709173 0.3747756 0.3645088 0.3080024 0.4362794
       [2,] 0.3094088 0.3482068 0.3434428 0.3640326 0.4125146
       [3,] 0.2352705 0.3025652 0.3443278 0.3355290 0.3767652
       [4,] 0.3380354 0.4241205 0.3946202 0.3848462 0.4305746
       [5,] 0.2257843 0.2659261 0.2756704 0.2924023 0.3363838
       [6,] 0.3056635 0.4055969 0.4240096 0.3766813 0.4993309
       [7,] 0.2402585 0.3187351 0.3299400 0.3508378 0.3431162
       [8,] 0.2370985 0.3102639 0.3191465 0.3091127 0.3742521
       [9,] 0.3357440 0.4125572 0.4189078 0.3884269 0.5217239
      [10,] 0.2703925 0.3560512 0.3061028 0.3297345 0.3386285
      [11,] 0.2874996 0.3232770 0.3536871 0.3310730 0.4284365
      [12,] 0.2988993 0.4074992 0.3836219 0.3640645 0.4327775
      [13,] 0.1794404 0.2703437 0.3285568 0.3208774 0.2919186
      [14,] 0.2905320 0.3675738 0.3830229 0.3476835 0.4346810
      [15,] 0.2544942 0.3423002 0.3477659 0.3423561 0.3966873
      [16,] 0.3001889 0.3730642 0.3961409 0.3343108 0.4149241
      [17,] 0.3109006 0.3483797 0.3230067 0.3173556 0.3552640
      [18,] 0.2631206 0.3060860 0.3213942 0.3336023 0.3474666
      [19,] 0.2796083 0.3134191 0.3424865 0.2765495 0.4134704
      [20,] 0.2363907 0.3240729 0.3367813 0.3222561 0.3821023
      [21,] 0.2927193 0.3292184 0.3246403 0.3275091 0.3961611
      [22,] 0.3627135 0.4269345 0.3923521 0.3864728 0.5149685
      [23,] 0.2818273 0.3464054 0.3149255 0.3746239 0.3622524
      [24,] 0.3044356 0.3738811 0.3712310 0.3510428 0.4349479
      [25,] 0.2729893 0.3354771 0.3314089 0.3499524 0.3730711
      [26,] 0.4067361 0.4384104 0.4082615 0.3858336 0.4803461
      [27,] 0.2578816 0.3105591 0.3153282 0.3302087 0.3754398
      [28,] 0.2968476 0.3670421 0.3489113 0.3543242 0.3987842
      [29,] 0.3992034 0.4197093 0.4048258 0.3608347 0.4386672
      [30,] 0.3401047 0.4377885 0.4059210 0.4316374 0.4426953
      [31,] 0.2711894 0.3444593 0.3258159 0.3462349 0.3904327
      [32,] 0.2902113 0.3202151 0.2927319 0.3132448 0.3934714
      [33,] 0.2902643 0.3562870 0.4211501 0.3930252 0.4677351
      [34,] 0.2515062 0.3131996 0.3429983 0.3068435 0.4012890
      [35,] 0.2591211 0.3377408 0.2983918 0.3068626 0.3686771
      [36,] 0.4220373 0.3496548 0.2905701 0.2660560 0.4066057
      [37,] 0.3496548 0.4139086 0.3615792 0.3398438 0.4231177
      [38,] 0.2905701 0.3615792 0.4243788 0.3349206 0.4348109
      [39,] 0.2660560 0.3398438 0.3349206 0.3967751 0.3664759
      [40,] 0.4066057 0.4231177 0.4348109 0.3664759 0.6118455

# `mi()` examples work

    Code
      mil_data <- generate_mild_df(positive_degree = 3, nbag = 10)
      with(mil_data, head(mi(bag_label, bag_name)))
    Output
           bag_label bag_name
      [1,] "1"       "bag1"  
      [2,] "1"       "bag1"  
      [3,] "1"       "bag1"  
      [4,] "1"       "bag1"  
      [5,] "1"       "bag1"  
      [6,] "1"       "bag1"  
    Code
      df <- get_all_vars(mi(bag_label, bag_name) ~ X1 + X2, data = mil_data)
      head(df)
    Output
        bag_label bag_name         X1         X2
      1         1     bag1 -0.4464099  1.1019817
      2         1     bag1  1.2311306 -0.8395224
      3         1     bag1  0.9309844  2.2884239
      4         1     bag1  2.0311929  2.1704369
      5         1     bag1  0.5209816 -1.1255467
      6         1     bag1 -0.4920105  2.4251912

# `mild_df()` examples work

    Code
      mild_df(bag_label = factor(c(1, 1, 0)), bag_name = c(rep("bag_1", 2), "bag_2"),
      instance_name = c("bag_1_inst_1", "bag_1_inst_2", "bag_2_inst_1"), X1 = c(-0.4,
        0.5, 2), instance_label = c(0, 1, 0))
    Output
        bag_label bag_name instance_name   X1
      1         1    bag_1  bag_1_inst_1 -0.4
      2         1    bag_1  bag_1_inst_2  0.5
      3         0    bag_2  bag_2_inst_1  2.0

# `mild()` examples work

    Code
      mil_data <- generate_mild_df(positive_degree = 3, nbag = 10)
      with(mil_data, head(mild(bag_label, bag_name, instance_name)))
    Output
           bag_label bag_name instance_name
      [1,] "1"       "bag1"   "bag1inst1"  
      [2,] "1"       "bag1"   "bag1inst1"  
      [3,] "1"       "bag1"   "bag1inst1"  
      [4,] "1"       "bag1"   "bag1inst1"  
      [5,] "1"       "bag1"   "bag1inst1"  
      [6,] "1"       "bag1"   "bag1inst1"  
    Code
      df <- get_all_vars(mild(bag_label, bag_name) ~ X1 + X2, data = mil_data)
      head(df)
    Output
        bag_label bag_name          X1         X2
      1         1     bag1  0.36529504 -0.3713889
      2         1     bag1  0.07733708 -0.5885037
      3         1     bag1 -1.59611098  0.3456669
      4         1     bag1 -0.16911543  1.4814259
      5         1     bag1 -0.85251086 -2.6362243
      6         1     bag1  1.40033082 -1.1679564

# `mior()` examples work

    Code
      set.seed(8)
      n <- 15
      X <- rbind(mvtnorm::rmvnorm(n / 3, mean = c(4, -2, 0)), mvtnorm::rmvnorm(n / 3,
      mean = c(0, 0, 0)), mvtnorm::rmvnorm(n / 3, mean = c(-2, 1, 0)))
      score <- X %*% c(2, -1, 0)
      y <- as.numeric(cut(score, c(-Inf, quantile(score, probs = 1:2 / 3), Inf)))
      bags <- 1:length(y)
      X <- rbind(X, mvtnorm::rmvnorm(n, mean = c(6, -3, 0)), mvtnorm::rmvnorm(n,
        mean = c(-6, 3, 0)))
      y <- c(y, rep(-1, 2 * n))
      bags <- rep(bags, 3)
      repr <- c(rep(1, n), rep(0, 2 * n))
      y_bag <- classify_bags(y, bags, condense = FALSE)
      mdl1 <- mior(X, y_bag, bags)
    Message <message>
      [Step 1] The optimization solution suggests that two intercepts are equal: b[1] == b[2].
      [Step 1] The optimization solution suggests that two intercepts are equal: b[2] == b[3].
    Warning <warning>
      [Step 1] There were NA values in `b`.  Replacing with 0.
    Message <message>
      [Step 2] The optimization solution suggests that two intercepts are equal: b[1] == b[2].
      [Step 2] The optimization solution suggests that two intercepts are equal: b[2] == b[3].
    Warning <warning>
      [Step 2] There were NA values in `b`.  Replacing with 0.
    Message <message>
      [Step 3] The optimization solution suggests that two intercepts are equal: b[2] == b[3].
    Warning <warning>
      [Step 3] There were NA values in `b`.  Replacing with 0.
    Code
      predict(mdl1, X, new_bags = bags)
    Output
      # A tibble: 45 x 1
         .pred_class
         <fct>      
       1 3          
       2 3          
       3 1          
       4 3          
       5 1          
       6 1          
       7 3          
       8 3          
       9 3          
      10 1          
      # ... with 35 more rows
    Code
      df1 <- bind_cols(y = y_bag, bags = bags, as.data.frame(X))
      df1 %>% bind_cols(predict(mdl1, df1, new_bags = bags, type = "class")) %>%
        bind_cols(predict(mdl1, df1, new_bags = bags, type = "raw")) %>% distinct(y,
        bags, .pred_class, .pred)
    Output
         y bags .pred_class       .pred
      1  3    1           3  0.90124158
      2  3    2           3  1.04883866
      3  3    3           1 -0.49073166
      4  3    4           3  1.61050422
      5  3    5           1 -2.15203379
      6  1    6           1 -0.69258966
      7  2    7           3  1.14459624
      8  2    8           3  0.09219952
      9  2    9           3 -0.06936906
      10 2   10           1 -1.01712520
      11 1   11           1 -1.58425706
      12 1   12           1 -0.85540376
      13 2   13           1 -0.39651685
      14 1   14           1 -1.08037692
      15 1   15           3 -0.17273959

# `mismm()` example works

    Code
      set.seed(8)
      mil_data <- generate_mild_df(nbag = 15, nsample = 20, positive_prob = 0.15,
        sd_of_mean = rep(0.1, 3))
      mdl1 <- mismm(mil_data)
      mdl2 <- mismm(mild(bag_label, bag_name, instance_name) ~ X1 + X2 + X3, data = mil_data)
      if (require(gurobi)) {
        mdl3 <- mismm(mil_data, method = "mip", control = list(nystrom_args = list(m = 10,
          r = 10)))
        predict(mdl3, mil_data)
      }
    Output
      # A tibble: 1,200 x 1
         .pred_class
         <fct>      
       1 0          
       2 0          
       3 0          
       4 0          
       5 0          
       6 0          
       7 0          
       8 0          
       9 0          
      10 0          
      # ... with 1,190 more rows
    Code
      predict(mdl1, new_data = mil_data, type = "raw", layer = "bag")
    Output
      # A tibble: 1,200 x 1
          .pred
          <dbl>
       1 -0.289
       2 -0.289
       3 -0.289
       4 -0.289
       5 -0.289
       6 -0.289
       7 -0.289
       8 -0.289
       9 -0.289
      10 -0.289
      # ... with 1,190 more rows
    Code
      mil_data %>% bind_cols(predict(mdl2, mil_data, type = "class")) %>% bind_cols(
        predict(mdl2, mil_data, type = "raw")) %>% distinct(bag_name, bag_label,
        .pred_class, .pred)
    Output
         bag_label bag_name .pred_class       .pred
      1          0     bag1           0 -0.11956070
      2          1     bag2           1  0.21090014
      3          0     bag3           0 -0.09390996
      4          1     bag4           1  0.09450872
      5          0     bag5           0 -0.09219624
      6          0     bag6           0 -0.13385229
      7          1     bag7           1  0.15464759
      8          0     bag8           0 -0.04083047
      9          1     bag9           1  0.16941904
      10         1    bag10           1  0.25006646
      11         1    bag11           1  0.13690982
      12         0    bag12           0 -0.05835921
      13         1    bag13           1  0.17325203
      14         1    bag14           1  0.14325177
      15         1    bag15           1  0.32877489

# `predict.mismm()` examples work

    Code
      mil_data <- generate_mild_df(nbag = 15, nsample = 20, positive_prob = 0.15,
        sd_of_mean = rep(0.1, 3))
      mdl1 <- mismm(mil_data, control = list(sigma = 1 / 5))
      mil_data %>% bind_cols(predict(mdl1, mil_data, type = "class")) %>% bind_cols(
        predict(mdl1, mil_data, type = "raw")) %>% distinct(bag_name, bag_label,
        .pred_class, .pred)
    Output
         bag_label bag_name .pred_class       .pred
      1          0     bag1           0 -0.37740255
      2          1     bag2           1  0.28344679
      3          0     bag3           0 -0.33199064
      4          1     bag4           1  0.13194961
      5          0     bag5           0 -0.33495785
      6          0     bag6           0 -0.24794687
      7          1     bag7           1  0.26145917
      8          0     bag8           0 -0.06040958
      9          1     bag9           1  0.37934673
      10         1    bag10           1  0.39238521
      11         1    bag11           1  0.30107408
      12         0    bag12           0 -0.28196329
      13         1    bag13           1  0.32638769
      14         1    bag14           1  0.22273935
      15         1    bag15           1  0.45906821
    Code
      mil_data %>% bind_cols(predict(mdl1, mil_data, type = "class", layer = "instance")) %>%
        bind_cols(predict(mdl1, mil_data, type = "raw", layer = "instance")) %>%
        distinct(bag_name, instance_name, bag_label, .pred_class, .pred)
    Output
         bag_label bag_name instance_name .pred_class       .pred
      1          0     bag1     bag1inst1           0 -0.38021259
      2          0     bag1     bag1inst2           0 -0.42078066
      3          0     bag1     bag1inst3           0 -0.37740255
      4          0     bag1     bag1inst4           0 -0.42800397
      5          1     bag2     bag2inst1           0 -0.32139888
      6          1     bag2     bag2inst2           0 -0.36278924
      7          1     bag2     bag2inst3           0 -0.25074180
      8          1     bag2     bag2inst4           1  0.28344679
      9          0     bag3     bag3inst1           0 -0.33199064
      10         0     bag3     bag3inst2           0 -0.39525382
      11         0     bag3     bag3inst3           0 -0.34569461
      12         0     bag3     bag3inst4           0 -0.33304887
      13         1     bag4     bag4inst1           0 -0.31900998
      14         1     bag4     bag4inst2           0 -0.24545506
      15         1     bag4     bag4inst3           0 -0.22103767
      16         1     bag4     bag4inst4           1  0.13194961
      17         0     bag5     bag5inst1           0 -0.45906816
      18         0     bag5     bag5inst2           0 -0.44501101
      19         0     bag5     bag5inst3           0 -0.33495785
      20         0     bag5     bag5inst4           0 -0.40857446
      21         0     bag6     bag6inst1           0 -0.39161068
      22         0     bag6     bag6inst2           0 -0.30728351
      23         0     bag6     bag6inst3           0 -0.24794687
      24         0     bag6     bag6inst4           0 -0.40984793
      25         1     bag7     bag7inst1           0 -0.22847389
      26         1     bag7     bag7inst2           1  0.26145917
      27         1     bag7     bag7inst3           0 -0.31135055
      28         1     bag7     bag7inst4           0 -0.26689426
      29         0     bag8     bag8inst1           0 -0.06040958
      30         0     bag8     bag8inst2           0 -0.31502077
      31         0     bag8     bag8inst3           0 -0.43158096
      32         0     bag8     bag8inst4           0 -0.32769429
      33         1     bag9     bag9inst1           1  0.37934673
      34         1     bag9     bag9inst2           0 -0.25656688
      35         1     bag9     bag9inst3           0 -0.15597718
      36         1     bag9     bag9inst4           0 -0.31327836
      37         1    bag10    bag10inst1           1  0.39238521
      38         1    bag10    bag10inst2           1  0.37351991
      39         1    bag10    bag10inst3           0 -0.22796150
      40         1    bag10    bag10inst4           0 -0.30594715
      41         1    bag11    bag11inst1           1  0.30107408
      42         1    bag11    bag11inst2           0 -0.25283830
      43         1    bag11    bag11inst3           0 -0.31290676
      44         1    bag11    bag11inst4           1  0.10095951
      45         0    bag12    bag12inst1           0 -0.39928055
      46         0    bag12    bag12inst2           0 -0.34958706
      47         0    bag12    bag12inst3           0 -0.28196329
      48         0    bag12    bag12inst4           0 -0.35179512
      49         1    bag13    bag13inst1           1  0.32638769
      50         1    bag13    bag13inst2           0 -0.25396081
      51         1    bag13    bag13inst3           0 -0.41066831
      52         1    bag13    bag13inst4           0 -0.36702806
      53         1    bag14    bag14inst1           0 -0.27036333
      54         1    bag14    bag14inst2           0 -0.33179443
      55         1    bag14    bag14inst3           1  0.22273935
      56         1    bag14    bag14inst4           0 -0.34223518
      57         1    bag15    bag15inst1           0 -0.30562584
      58         1    bag15    bag15inst2           0 -0.23537016
      59         1    bag15    bag15inst3           0 -0.21896721
      60         1    bag15    bag15inst4           1  0.45906821

# `misvm_orova()` examples work

    Code
      set.seed(8)
      n <- 500
      y <- sample(1:5, size = n, prob = (1 / 1:5)^2, replace = TRUE)
      bags <- rep(1:(n / 5), each = 5)
      X <- matrix(NA, nrow = length(y), ncol = 5)
      for (y_ in unique(y)) {
        to_fill <- which(y_ == y)
        X[to_fill, ] <- mvtnorm::rmvnorm(length(to_fill), mean = c(2 * y_, -1 * y_,
        1 * y_, 0, 0))
      }
      colnames(X) <- paste0("V", 1:ncol(X))
      y <- classify_bags(y, bags, condense = FALSE)
      mdl1 <- misvm_orova(X, y, bags)
      predict(mdl1, X, new_bags = bags)
    Output
      # A tibble: 500 x 1
         .pred_class
         <fct>      
       1 2          
       2 2          
       3 2          
       4 2          
       5 2          
       6 4          
       7 4          
       8 4          
       9 4          
      10 4          
      # ... with 490 more rows
    Code
      df1 <- bind_cols(y = y, bags = bags, as.data.frame(X))
      df1 %>% bind_cols(predict(mdl1, df1, new_bags = bags, type = "class")) %>%
        bind_cols(predict(mdl1, df1, new_bags = bags, type = "raw")) %>% select(
        -starts_with("V")) %>% distinct()
    Output
          y bags .pred_class     .pred_1     .pred_2     .pred_3      .pred_4
      1   2    1           2  1.00360442  2.41019969 -1.18253233 -1.512211726
      2   4    2           4  0.18473576  0.97538044  0.87628566  1.000170075
      3   1    3           2  0.41243016  0.86917258 -1.44928421 -1.980658135
      4   3    4           3  0.09688308  0.50607311  1.00038027  0.298939913
      5   1    5           2  1.68503843  1.91105245 -0.55051239 -1.555077577
      6   5    6           4  0.80288083 -0.67309718  2.82080117  3.022573640
      7   4    7           2  1.03176176  2.73949683  2.73415205  2.190589612
      8   4    8           3 -0.12854362  0.26319205  2.46768202  2.142009140
      9   3    9           1  1.05320870  0.38559361  0.84031431  0.212133906
      10  4   10           3  0.26274766  0.74397583  3.57406876  3.158620363
      11  5   11           4  0.11558826  0.78138565  3.78746699  3.916484935
      12  3   12           1  0.91028443  0.29300684  0.28268951 -0.201718489
      13  5   13           3 -0.93663663 -0.06659367  3.33943483  2.139197061
      14  3   14           3 -0.28346120 -0.48213726  1.00000352  0.582401536
      15  2   15           2 -0.56823729  0.30834149 -1.36066042 -1.673639727
      16  3   16           3 -0.49114696  1.06429250  1.75111280  1.307331901
      17  2   17           2  0.77190394  1.46499851 -1.44623337 -2.158377377
      18  2   18           2  1.19430637  1.72536105 -0.49537827 -1.488194667
      19  2   19           2 -0.13803535  0.99938455  0.43330652 -0.877719846
      20  1   20           1  1.75913902  1.42754817 -0.65558342 -2.162853555
      21  3   21           1  1.03942838  0.03626303  0.09839686  0.026249662
      22  2   22           2 -0.11675899  0.54917970 -0.74689429 -1.111492350
      23  3   23           3  0.12495445  1.01807078  1.18433770 -0.017122158
      24  4   24           3 -0.74294596 -0.55855080  2.39823597  2.163251488
      25  2   25           2  0.42969159  1.83624242 -0.41405639 -0.675596048
      26  4   26           3 -0.90608156 -0.76433331  1.11438539  0.999335671
      27  4   27           4 -1.12976852 -0.55397556  1.99235840  1.998571616
      28  2   28           3  0.51065530  0.73065242  1.01307271 -0.786797608
      29  5   29           4  1.02422924  0.51212543  3.54074733  3.626395035
      30  3   30           2 -1.15825166  2.05452322  1.59826250  1.981821403
      31  2   31           1  1.22914412  1.13607325 -0.49895099 -1.634086726
      32  5   32           4  2.24781984  1.12639223  2.46671952  2.672952814
      33  2   33           2  0.56762884  3.21045618 -0.14177894 -0.836467395
      34  5   34           3  1.13477571  0.02553029  3.21147326  3.100541800
      35  4   35           3  0.28548736 -0.04810715  1.82746796  1.805530537
      36  3   36           2 -0.98106965  1.57910422  0.86021057  0.353531718
      37  4   37           3 -0.51805860  0.49430869  1.20768580  1.000049847
      38  2   38           2  0.56740602  1.00022042 -0.28521527 -0.002722615
      39  4   39           3  0.80784405  0.28450534  1.67505556  1.435226358
      40  2   40           3  0.34756236 -0.02840180  0.64466340 -0.235389840
      41  1   41           2  0.23565438  1.79585508 -1.52922714 -2.483746155
      42  1   42           1  1.52970463  1.48852510 -0.66886078 -2.224293497
      43  2   43           2  1.28403512  2.27811274  0.67044399  0.301873983
      44  2   44           1  0.85903892  0.25285456 -0.42730581 -0.477324163
      45  2   45           1  0.66506642 -0.62817026  0.08698070 -0.167220296
      46  3   46           1  1.56955123  1.54055532  1.21693354  1.164472761
      47  3   47           3  0.37913742 -0.13979978  0.82903773  0.073971711
      48  4   48           4  0.45114372 -0.37020885  2.36676210  2.665439494
      49  2   49           2  0.91561725  1.28085362 -0.18601085 -0.709600617
      50  4   50           3 -0.84201397  1.49894952  1.84527148  1.000284104
      51  3   51           3 -0.68905169  0.05692769  1.02322487  0.572320450
      52  5   52           3 -0.68626872  0.71565945  2.31057521  1.977668132
      53  5   53           3  0.87743879 -0.41844687  2.60735522  2.184017448
      54  2   54           2 -0.95509939  1.46918667 -0.42888095 -1.680365997
      55  4   55           3  0.89033890  1.36284335  2.46471277  1.786317360
      56  3   56           3  0.32521441 -0.24660493  1.58702341  0.173709842
      57  5   57           4  1.55462088  2.24532327  4.38461435  4.397476215
      58  1   58           1  1.06424156  0.26808936 -0.58310435 -1.531644186
      59  2   59           1  0.24000004  0.04221692  0.11696382 -0.866481818
      60  4   60           3  0.48505732  1.48958367  1.80652220  1.000049915
      61  2   61           2  1.53368322  1.54846350 -0.96795519 -1.087685551
      62  2   62           1  1.16303648  1.12410889 -1.06981294 -2.323188255
      63  2   63           1  0.59612111 -0.24277660 -0.43494122 -0.906534708
      64  5   64           4  0.20702590  0.69901408  3.55881572  3.669407750
      65  2   65           2 -0.34137240  0.88567695  0.03168834 -0.636341622
      66  4   66           4 -0.10809794  0.42091908  2.56146579  2.766309855
      67  2   67           2  0.92291099  1.41142690 -0.52136403 -1.529892257
      68  4   68           3  0.26041584 -0.34249460  1.69491581  1.180947173
      69  1   69           2  0.30527182  1.61344752 -1.62536509 -2.391720511
      70  2   70           2 -0.83316279  0.38165638 -0.53766426 -1.198709928
      71  4   71           4 -1.29146828  0.33308290  2.47073326  2.724127748
      72  4   72           4  0.56632536  0.05842956  1.25554974  1.631375803
      73  1   73           1  1.00042251  0.54078624 -1.10129027 -2.135775222
      74  5   74           4  1.57124744  0.89637011  3.45985441  3.648599280
      75  5   75           3 -0.16812174  0.74369294  4.09017620  3.740035820
      76  4   76           4 -0.31010636 -0.45663542  2.11400764  2.211934336
      77  1   77           1  1.16931845  0.88990004 -0.62792655 -1.561332059
      78  3   78           3  1.18980202  1.17686411  2.02809051  1.058778628
      79  2   79           3 -1.04089029  0.30735742  0.56830672  0.181003811
      80  1   80           1  0.99972236  0.61761910 -1.67637595 -2.456447225
      81  3   81           2  0.87091335  1.62238492  1.20096152 -0.123250665
      82  3   82           2  1.13240151  1.79326755  1.00000318  0.862251960
      83  2   83           1  0.95748104  0.72509995 -0.26110396 -0.759395738
      84  5   84           4  0.18614745 -0.19865597  3.56842740  3.687528308
      85  2   85           2 -0.14571732  1.48642591  0.22265141 -0.218651693
      86  2   86           2  0.43148647  0.93739007  0.37729566 -0.768011360
      87  3   87           3  0.25682955  0.26594945  1.37887800  1.046423678
      88  3   88           3  1.45453085  0.41931611  2.35970660  2.029739218
      89  3   89           3  0.59709661  1.46801235  1.51342514 -0.298124744
      90  2   90           2  1.67042606  2.91618485  0.32798460 -0.759462082
      91  3   91           4 -0.47140903  0.72069561  1.23552344  1.472915067
      92  4   92           2  0.47341945  3.22614926  2.13554456  1.532243047
      93  3   93           3  0.26893591  0.30789345  1.18613532 -0.160351025
      94  2   94           2 -1.42450714  0.10044644 -0.57169264 -1.457744502
      95  3   95           1  1.21818064  0.06412499  1.01482459 -0.451303533
      96  3   96           2 -1.08962634  1.63190056  0.39030420  0.382634228
      97  2   97           1  1.31130845 -0.01888615 -1.08295680 -1.113259303
      98  2   98           1  1.45320037  1.08381358  0.16081299 -1.708500754
      99  2   99           2  0.85614682  1.00019766 -0.36354153 -1.026853114
      100 2  100           2  0.67385413  1.26992716 -0.86277231 -1.815537750
               .pred_5
      1   -2.710575952
      2   -0.630536324
      3   -3.181207430
      4   -0.503732033
      5   -1.936185242
      6    1.770699122
      7    0.269616439
      8    0.609662132
      9   -0.568226688
      10   0.231024272
      11   2.072947941
      12  -1.824534063
      13   1.892459323
      14  -1.601215977
      15  -2.628482899
      16  -0.118811081
      17  -2.252933848
      18  -2.844889298
      19  -1.379658223
      20  -2.517565650
      21  -2.115844252
      22  -1.281168071
      23  -0.525694741
      24   0.208287431
      25  -2.546434548
      26  -0.437630508
      27   0.286879018
      28  -1.075342024
      29   1.170363733
      30   0.361835669
      31  -2.006331292
      32   0.999837007
      33  -1.744376225
      34   1.416721842
      35  -0.999928169
      36  -1.343717247
      37   0.138596510
      38  -2.446294863
      39  -0.007459032
      40  -1.521239169
      41  -2.889689945
      42  -2.894731975
      43  -0.466378575
      44  -2.339745262
      45  -1.172660639
      46  -0.403095749
      47  -1.031593928
      48   0.286996550
      49  -1.389482515
      50   0.672265043
      51  -0.185651273
      52   0.881630613
      53   1.689190261
      54  -1.420214538
      55   1.130385046
      56   0.014161245
      57   3.487036498
      58  -1.703313081
      59  -0.837590033
      60  -0.998126035
      61  -2.570229087
      62  -2.902641413
      63  -1.665414853
      64   1.532512762
      65  -1.793173808
      66   1.364966159
      67  -2.644312238
      68   1.032918446
      69  -2.551873594
      70  -2.566362850
      71   1.316496179
      72  -0.314994983
      73  -3.402412023
      74   1.770087234
      75   2.119349722
      76  -0.020697335
      77  -2.153330200
      78  -0.515141225
      79  -1.620389427
      80  -2.984748981
      81  -1.294072354
      82   0.002103730
      83  -1.580173185
      84   1.133354510
      85  -1.775242039
      86  -1.202950728
      87   0.049695886
      88  -0.267687391
      89  -0.508838948
      90  -1.986896227
      91  -1.820478565
      92  -0.566692413
      93  -0.045569860
      94  -2.586503541
      95  -0.680908005
      96  -1.217662731
      97  -3.464748544
      98  -1.581837088
      99  -2.040235487
      100 -1.931645358

# `misvm()` examples work

    Code
      set.seed(8)
      mil_data <- generate_mild_df(nbag = 20, positive_prob = 0.15, sd_of_mean = rep(
        0.1, 3))
      df <- build_instance_feature(mil_data, seq(0.05, 0.95, length.out = 10))
      mdl1 <- misvm(x = df[, 4:123], y = df$bag_label, bags = df$bag_name, method = "heuristic")
      mdl2 <- misvm(mi(bag_label, bag_name) ~ X1_mean + X2_mean + X3_mean, data = df)
      if (require(gurobi)) {
        mdl3 <- misvm(x = df[, 4:123], y = df$bag_label, bags = df$bag_name, method = "mip")
      }
      predict(mdl1, new_data = df, type = "raw", layer = "bag")
    Output
      # A tibble: 80 x 1
         .pred
         <dbl>
       1 -1.04
       2 -1.04
       3 -1.04
       4 -1.04
       5  1.00
       6  1.00
       7  1.00
       8  1.00
       9 -1.00
      10 -1.00
      # ... with 70 more rows
    Code
      df %>% bind_cols(predict(mdl2, df, type = "class")) %>% bind_cols(predict(mdl2,
        df, type = "raw")) %>% distinct(bag_name, bag_label, .pred_class, .pred)
    Output
         bag_label bag_name .pred_class       .pred
      1          0     bag1           0 -0.11805071
      2          1     bag2           1  1.01732791
      3          0     bag3           0 -0.24540426
      4          1     bag4           1  1.00046917
      5          0     bag5           1  0.15460188
      6          0     bag6           1  0.87469487
      7          1     bag7           1  0.16754553
      8          0     bag8           1  1.00811386
      9          1     bag9           1  0.99998275
      10         1    bag10           1  2.67168111
      11         1    bag11           1  0.29471379
      12         0    bag12           1  1.52487131
      13         1    bag13           1  2.15326561
      14         1    bag14           1  0.99956477
      15         1    bag15           0 -0.38940230
      16         1    bag16           1  0.67654218
      17         0    bag17           1  0.39241276
      18         1    bag18           0 -0.11878006
      19         0    bag19           1  0.06554383
      20         1    bag20           1  0.85951804

# `omisvm()` examples work

    Code
      set.seed(8)
      n <- 500
      y <- sample(1:5, size = n, prob = (1 / 1:5)^2, replace = TRUE)
      bags <- rep(1:(n / 5), each = 5)
      X <- matrix(NA, nrow = length(y), ncol = 5)
      for (y_ in unique(y)) {
        to_fill <- which(y_ == y)
        X[to_fill, ] <- mvtnorm::rmvnorm(length(to_fill), mean = c(2 * y_, -1 * y_,
        1 * y_, 0, 0))
      }
      colnames(X) <- paste0("V", 1:ncol(X))
      y <- classify_bags(y, bags, condense = FALSE)
      mdl1 <- omisvm(X, y, bags, weights = NULL)
      predict(mdl1, X, new_bags = bags)
    Output
      # A tibble: 500 x 1
         .pred_class
         <fct>      
       1 1          
       2 1          
       3 1          
       4 1          
       5 1          
       6 3          
       7 3          
       8 3          
       9 3          
      10 3          
      # ... with 490 more rows
    Code
      df1 <- bind_cols(y = y, bags = bags, as.data.frame(X))
      df1 %>% bind_cols(predict(mdl1, df1, new_bags = bags, type = "class")) %>%
        bind_cols(predict(mdl1, df1, new_bags = bags, type = "raw")) %>% distinct(y,
        bags, .pred_class, .pred)
    Output
          y bags .pred_class     .pred
      1   2    1           1  2.740123
      2   4    2           3  5.389850
      3   1    3           1  2.018423
      4   3    4           3  5.514886
      5   1    5           2  3.046329
      6   5    6           5  8.187854
      7   4    7           4  7.133660
      8   4    8           4  7.325424
      9   3    9           3  4.997991
      10  4   10           5  7.765274
      11  5   11           5  9.084129
      12  3   12           2  4.416985
      13  5   13           5  7.806662
      14  3   14           3  5.043793
      15  2   15           1  2.779580
      16  3   16           4  6.347557
      17  2   17           1  2.511643
      18  2   18           2  2.968020
      19  2   19           2  4.126820
      20  1   20           1  2.423266
      21  3   21           2  4.296094
      22  2   22           2  3.503202
      23  3   23           3  5.256088
      24  4   24           4  7.210210
      25  2   25           2  3.513103
      26  4   26           3  5.797886
      27  4   27           4  6.848268
      28  2   28           2  4.401413
      29  5   29           5  8.462649
      30  3   30           4  6.662208
      31  2   31           2  2.960173
      32  5   32           5  7.755559
      33  2   33           2  3.845076
      34  5   34           5  8.314954
      35  4   35           4  6.159100
      36  3   36           3  5.020061
      37  4   37           3  5.985965
      38  2   38           2  4.065930
      39  4   39           4  6.342460
      40  2   40           2  4.403038
      41  1   41           1  2.005765
      42  1   42           1  2.265122
      43  2   43           3  5.061454
      44  2   44           2  3.700449
      45  2   45           3  4.636903
      46  3   46           3  5.840668
      47  3   47           3  4.941359
      48  4   48           4  7.199078
      49  2   49           2  4.165544
      50  4   50           4  6.571696
      51  3   51           3  5.645460
      52  5   52           4  7.109256
      53  5   53           5  7.750269
      54  2   54           2  3.273363
      55  4   55           4  7.231633
      56  3   56           3  5.630279
      57  5   57           5 10.048513
      58  1   58           2  3.115462
      59  2   59           2  4.217701
      60  4   60           3  5.615296
      61  2   61           2  3.102841
      62  2   62           1  2.403945
      63  2   63           2  3.816636
      64  5   64           5  8.757338
      65  2   65           2  4.067196
      66  4   66           5  7.769631
      67  2   67           2  3.024283
      68  4   68           4  6.491761
      69  1   69           1  1.981162
      70  2   70           2  3.305330
      71  4   71           5  7.936683
      72  4   72           4  6.201439
      73  1   73           1  1.964279
      74  5   74           5  8.756221
      75  5   75           5  9.061753
      76  4   76           4  6.571696
      77  1   77           2  2.852790
      78  3   78           3  5.879681
      79  2   79           2  4.390224
      80  1   80           1  1.884108
      81  3   81           3  4.760278
      82  3   82           3  5.781256
      83  2   83           2  3.845076
      84  5   84           5  8.571696
      85  2   85           2  4.235719
      86  2   86           2  4.287354
      87  3   87           4  6.058172
      88  3   88           4  6.594769
      89  3   89           3  5.169389
      90  2   90           2  3.900907
      91  3   91           3  5.669294
      92  4   92           4  6.213718
      93  3   93           3  5.360515
      94  2   94           2  3.077553
      95  3   95           3  4.926517
      96  3   96           2  4.379651
      97  2   97           1  2.733166
      98  2   98           2  3.589258
      99  2   99           2  3.595041
      100 2  100           2  2.887636

# `smm()` examples work

    Code
      set.seed(8)
      n_instances <- 10
      n_samples <- 20
      y <- rep(c(1, -1), each = n_samples * n_instances / 2)
      instances <- as.character(rep(1:n_instances, each = n_samples))
      x <- data.frame(x1 = rnorm(length(y), mean = 1 * (y == 1)), x2 = rnorm(length(y),
      mean = 2 * (y == 1)), x3 = rnorm(length(y), mean = 3 * (y == 1)))
      df <- data.frame(instance_name = instances, y = y, x)
      mdl <- smm(x, y, instances)
      mdl2 <- smm(y ~ ., data = df)
      df %>% dplyr::bind_cols(predict(mdl, type = "raw", new_data = x, new_instances = instances)) %>%
        dplyr::bind_cols(predict(mdl, type = "class", new_data = x, new_instances = instances)) %>%
        dplyr::distinct(instance_name, y, .pred, .pred_class)
    Output
         instance_name  y      .pred .pred_class
      1              1  1  1.0000000           1
      2              2  1  0.9038444           1
      3              3  1  1.1047533           1
      4              4  1  0.9112317           1
      5              5  1  0.8965109           1
      6              6 -1 -1.1437942          -1
      7              7 -1 -1.0000000          -1
      8              8 -1 -1.0679171          -1
      9              9 -1 -1.1645562          -1
      10            10 -1 -1.2293499          -1

# `summarize_samples()` examples work

    Code
      fns <- list(mean = mean, sd = sd)
      suppressMessages({
        summarize_samples(mtcars, group_cols = c("cyl", "gear"), .fns = fns) %>%
          print()
        summarize_samples(mtcars, group_cols = c("cyl", "gear"), .fns = fns, cor = TRUE) %>%
          print()
      })
    Output
      # A tibble: 8 x 20
          cyl  gear mpg_mean disp_mean hp_mean drat_mean wt_mean qsec_mean vs_mean
        <dbl> <dbl>    <dbl>     <dbl>   <dbl>     <dbl>   <dbl>     <dbl>   <dbl>
      1     4     3     21.5      120.     97       3.7     2.46      20.0     1  
      2     4     4     26.9      103.     76       4.11    2.38      19.6     1  
      3     4     5     28.2      108.    102       4.1     1.83      16.8     0.5
      4     6     3     19.8      242.    108.      2.92    3.34      19.8     1  
      5     6     4     19.8      164.    116.      3.91    3.09      17.7     0.5
      6     6     5     19.7      145     175       3.62    2.77      15.5     0  
      7     8     3     15.0      358.    194.      3.12    4.10      17.1     0  
      8     8     5     15.4      326     300.      3.88    3.37      14.6     0  
      # ... with 11 more variables: am_mean <dbl>, carb_mean <dbl>, mpg_sd <dbl>,
      #   disp_sd <dbl>, hp_sd <dbl>, drat_sd <dbl>, wt_sd <dbl>, qsec_sd <dbl>,
      #   vs_sd <dbl>, am_sd <dbl>, carb_sd <dbl>
      # A tibble: 8 x 56
          cyl  gear mpg_mean disp_mean hp_mean drat_mean wt_mean qsec_mean vs_mean
        <dbl> <dbl>    <dbl>     <dbl>   <dbl>     <dbl>   <dbl>     <dbl>   <dbl>
      1     4     3     21.5      120.     97       3.7     2.46      20.0     1  
      2     4     4     26.9      103.     76       4.11    2.38      19.6     1  
      3     4     5     28.2      108.    102       4.1     1.83      16.8     0.5
      4     6     3     19.8      242.    108.      2.92    3.34      19.8     1  
      5     6     4     19.8      164.    116.      3.91    3.09      17.7     0.5
      6     6     5     19.7      145     175       3.62    2.77      15.5     0  
      7     8     3     15.0      358.    194.      3.12    4.10      17.1     0  
      8     8     5     15.4      326     300.      3.88    3.37      14.6     0  
      # ... with 47 more variables: am_mean <dbl>, carb_mean <dbl>, mpg_sd <dbl>,
      #   disp_sd <dbl>, hp_sd <dbl>, drat_sd <dbl>, wt_sd <dbl>, qsec_sd <dbl>,
      #   vs_sd <dbl>, am_sd <dbl>, carb_sd <dbl>, cov_var_1 <dbl>, cov_var_2 <dbl>,
      #   cov_var_3 <dbl>, cov_var_4 <dbl>, cov_var_5 <dbl>, cov_var_6 <dbl>,
      #   cov_var_7 <dbl>, cov_var_8 <dbl>, cov_var_9 <dbl>, cov_var_10 <dbl>,
      #   cov_var_11 <dbl>, cov_var_12 <dbl>, cov_var_13 <dbl>, cov_var_14 <dbl>,
      #   cov_var_15 <dbl>, cov_var_16 <dbl>, cov_var_17 <dbl>, cov_var_18 <dbl>, ...

# `svor_exc()` examples work

    Code
      set.seed(8)
      n <- 400
      y <- sample(1:5, size = n, prob = (1 / 1:5), replace = TRUE)
      X <- matrix(NA, nrow = length(y), ncol = 5)
      for (y_ in unique(y)) {
        to_fill <- which(y_ == y)
        X[to_fill, ] <- mvtnorm::rmvnorm(length(to_fill), mean = c(2 * y_, -1 * y_,
        1 * y_, 0, 0))
      }
      colnames(X) <- paste0("V", 1:ncol(X))
      mdl1 <- svor_exc(X, y)
    Message <message>
      The SMO algorithm reached the maximum of 500 steps.
    Code
      predict(mdl1, X)
    Output
      # A tibble: 400 x 1
         .pred_class
         <fct>      
       1 2          
       2 1          
       3 3          
       4 2          
       5 2          
       6 3          
       7 1          
       8 5          
       9 3          
      10 2          
      # ... with 390 more rows
    Code
      predict(mdl1, X, type = "raw")
    Output
      # A tibble: 400 x 1
           .pred
           <dbl>
       1 -0.361 
       2 -4.63  
       3  1.95  
       4  0.0859
       5 -1.39  
       6  2.98  
       7 -4.49  
       8  6.40  
       9  3.10  
      10 -0.415 
      # ... with 390 more rows

